{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36508ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=nlu_sentiment_analysis\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/steve/.netrc\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=nlu_sentiment_analysis\n",
    "!wandb login 2cad8a8279143c69ce071f54bf37c1f5a5f4e5ff\n",
    "import wandb\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import requests, re, string, datetime, copy\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T, torch.nn.functional as F, torch.nn as nn\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback, TrainerCallback\n",
    "from transformers import Trainer\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "PATH = \"./data/Sentipolc16/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252c576e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idtwitter</th>\n",
       "      <th>subj</th>\n",
       "      <th>opos</th>\n",
       "      <th>oneg</th>\n",
       "      <th>iro</th>\n",
       "      <th>lpos</th>\n",
       "      <th>lneg</th>\n",
       "      <th>top</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122449983151669248</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Intanto la partita per Via Nazionale si compli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125485104863780865</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False illusioni, sgradevoli realtà Mario Monti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125513454315507712</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False illusioni, sgradevoli realtà #editoriale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125524238290522113</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mario Monti: Berlusconi risparmi all'Italia il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125527933224886272</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mario Monti: Berlusconi risparmi all'Italia il...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            idtwitter  subj  opos  oneg  iro  lpos  lneg  top  \\\n",
       "0  122449983151669248     1     0     1    0     0     1    1   \n",
       "1  125485104863780865     1     0     1    0     0     1    1   \n",
       "2  125513454315507712     1     0     1    0     0     1    1   \n",
       "3  125524238290522113     1     0     1    0     0     1    1   \n",
       "4  125527933224886272     1     0     1    0     0     1    1   \n",
       "\n",
       "                                                text  \n",
       "0  Intanto la partita per Via Nazionale si compli...  \n",
       "1  False illusioni, sgradevoli realtà Mario Monti...  \n",
       "2  False illusioni, sgradevoli realtà #editoriale...  \n",
       "3  Mario Monti: Berlusconi risparmi all'Italia il...  \n",
       "4  Mario Monti: Berlusconi risparmi all'Italia il...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(PATH + \"training_set_sentipolc16.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb8180d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idtwitter</th>\n",
       "      <th>subj</th>\n",
       "      <th>opos</th>\n",
       "      <th>oneg</th>\n",
       "      <th>iro</th>\n",
       "      <th>lpos</th>\n",
       "      <th>lneg</th>\n",
       "      <th>top</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>507074506880712705</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>\"Tra 5 minuti presentazione piano scuola del g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>507075789456961536</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>\"\\\"@matteorenzi: Alle 10 appuntamento su http:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>507077511902425088</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>\"#labuonascuola gli #evangelisti #digitali non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>507079183315787777</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>\"Riforma scuola Tutto il discorso di  Renzi su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>507080190225563648</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>\".@matteorenzi @MiurSocial #labuonascuola bast...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            idtwitter  subj  opos  oneg  iro  lpos  lneg  top  \\\n",
       "0  507074506880712705     0     0     0    0     0     0    2   \n",
       "1  507075789456961536     1     1     0    0     1     0    2   \n",
       "2  507077511902425088     1     0     1    0     0     1    2   \n",
       "3  507079183315787777     0     0     0    0     0     0    2   \n",
       "4  507080190225563648     1     0     0    0     0     0    2   \n",
       "\n",
       "                                                text  \n",
       "0  \"Tra 5 minuti presentazione piano scuola del g...  \n",
       "1  \"\\\"@matteorenzi: Alle 10 appuntamento su http:...  \n",
       "2  \"#labuonascuola gli #evangelisti #digitali non...  \n",
       "3  \"Riforma scuola Tutto il discorso di  Renzi su...  \n",
       "4  \".@matteorenzi @MiurSocial #labuonascuola bast...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = open(PATH + \"test_set_sentipolc16_gold2000.csv\", 'r') \n",
    "Lines = file1.readlines()\n",
    " \n",
    "test = []\n",
    "for line in Lines:\n",
    "  arr = line.split(\"\\\",\")\n",
    "  if len(arr) != 9:\n",
    "    arr[8] = arr[8] + arr[9]  #to account for tweets containing the delimiter charachter that would create more splits than needed\n",
    "    del arr[9:]\n",
    "  for i in range(8):\n",
    "    arr[i] = int(arr[i].strip(\"\\\"\"))\n",
    "  test.append(arr)\n",
    "\n",
    "test = pd.DataFrame(test, columns=train.columns)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc5af1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steve/anaconda3/envs/nlu/lib/python3.7/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/steve/anaconda3/envs/nlu/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "<hashtag> il governo </hashtag> presenta le linee guida sulla scuola <hashtag> la buona scuola </hashtag> <url>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tokenization classes for Italian AlBERTo models.\"\"\"\n",
    "import collections\n",
    "import os\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip(\"\\n\")\n",
    "        vocab[token] = index\n",
    "    return vocab\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'user', 'percent', 'money', 'phone', 'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\"},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "\n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "class AlBERTo_Preprocessing(object):\n",
    "    def __init__(self, do_lower_case=True, **kwargs):\n",
    "        self.do_lower_case = do_lower_case\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if self.do_lower_case:\n",
    "            text = text.lower()\n",
    "        text = str(\" \".join(text_processor.pre_process_doc(text)))\n",
    "        text = re.sub(r'[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
    "        text = re.sub(r'^\\s', '', text)\n",
    "        text = re.sub(r'\\s$', '', text)\n",
    "        return text\n",
    "\n",
    "a = AlBERTo_Preprocessing(do_lower_case=True)\n",
    "s = \"#IlGOverno presenta le linee guida sulla scuola #labuonascuola - http://t.co/SYS1T9QmQN\"\n",
    "b = a.preprocess(s)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d876ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_steps_evidence(num_iterations, early_stopping_patience, training_args, net, training, testing, validating):\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    keys = [\"eval_loss\", \"eval_accuracy\", \"eval_f1\", \"eval_precision\", \"eval_recall\"]\n",
    "    metrics = { i: [] for i in keys}\n",
    "    for i in range(num_iterations):        \n",
    "        trainer = Trainer(\n",
    "            model=net(3),\n",
    "            args=training_args, \n",
    "            train_dataset=training.shuffle(seed=i), \n",
    "            eval_dataset=validating, \n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)] if early_stopping_patience is not None else None\n",
    "        )\n",
    "        trainer.train()\n",
    "        res = trainer.evaluate(testing)\n",
    "        for m in keys:\n",
    "            metrics[m].append(res[m])\n",
    "\n",
    "    for m in metrics.keys():\n",
    "        print(\"{:18s}\\t: {:.4} ± {:.4}\".format(m, np.mean(metrics[m]), np.std(metrics[m])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7af16a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'ha', '##shtag', '>', 'il', 'governo', '<', '/', 'ha', '##shtag', '>', 'presenta', 'le', 'linee', 'guida', 'sulla', 'scuola', '<', 'ha', '##shtag', '>', 'la', 'buona', 'scuola', '<', '/', 'ha', '##shtag', '>', '<', 'ur', '##l', '>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "a = AlBERTo_Preprocessing(do_lower_case=True)\n",
    "s: str = \"#IlGOverno presenta le linee guida sulla scuola #labuonascuola - http://t.co/SYS1T9QmQN\"\n",
    "b = a.preprocess(s)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "pretrained_model = AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "tok.model_max_length = 128 #model.config.max_position_embeddings\n",
    "tokens = tok.tokenize(b)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd96fda6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a2676b40184c5da41e08b57f167a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b58d0bd10c04ca59aad3a42ecceb6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da3d2b6e0494b3ba3a048f807bef692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7771a512fbf944febb3348833e40b33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a403de13b44e50ae10245ab9f56c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d36a38e273472cadee9af8b72a4e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    sa = tok(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    return sa\n",
    "\n",
    "def separate2united_labels(row):\n",
    "    \"\"\"\n",
    "        Return a single scalar integer label associated to the polarity of the tweet.\n",
    "\n",
    "        Negative -> 0\n",
    "        Neutral  -> 1\n",
    "        Positive -> 2\n",
    "        Mixed    -> 3\n",
    "    \"\"\"\n",
    "    if row[\"opos\"] == 0 and row[\"oneg\"] == 0:\n",
    "        return 1\n",
    "    elif row[\"oneg\"] == 0 and row[\"opos\"] == 1:\n",
    "        return 2\n",
    "    elif row[\"oneg\"] == 1 and row[\"opos\"] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "#train set\n",
    "dataset = pd.DataFrame({\"text\": train.text.apply(a.preprocess), \"idx\": train.index, \"labels\": train[[\"opos\", \"oneg\"]].apply(separate2united_labels, axis=1)})\n",
    "X_train, X_val = train_test_split(dataset, test_size=0.2, random_state=42, stratify=dataset[\"labels\"])\n",
    "\n",
    "X_train = Dataset.from_pandas(X_train)\n",
    "X_val = Dataset.from_pandas(X_val)\n",
    "\n",
    "training = X_train\\\n",
    "                    .map(tokenize_function, batched=True)\\\n",
    "                    .filter(lambda example: example['labels'] != 3)\\\n",
    "                    .shuffle(seed=42)\\\n",
    "                    .with_format(\"torch\")\n",
    "validating = X_val\\\n",
    "                    .map(tokenize_function, batched=True)\\\n",
    "                    .filter(lambda example: example['labels'] != 3)\\\n",
    "                    .with_format(\"torch\")\n",
    "\n",
    "\n",
    "#test set\n",
    "dataset = pd.DataFrame({\"text\": test.text.apply(a.preprocess), \"idx\": test.index, \"labels\": test[[\"opos\", \"oneg\"]].apply(separate2united_labels, axis=1)})\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "testing = dataset\\\n",
    "                    .map(tokenize_function, batched=True)\\\n",
    "                    .filter(lambda example: example['labels'] != 3)\\\n",
    "                    .with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67f630d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, f1_score\n",
    "\n",
    "class EarlyStopping():    \n",
    "    def __init__(self, min_delta = 0, patience = 0):        \n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = -np.Inf\n",
    "        self.stop_training = False\n",
    "    \n",
    "    def on_epoch_end(self, epoch, current_value):\n",
    "        if np.greater((current_value - self.min_delta), self.best):\n",
    "            self.best = current_value\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait > self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.stop_training = True\n",
    "        return self.stop_training\n",
    "\n",
    "\n",
    "TRAIN_BATCH_SIZE = 64 \n",
    "PREDICT_BATCH_SIZE = 64\n",
    "EVAL_BATCH_SIZE = 64 \n",
    "WEIGHT_DECAY = 0.01\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 128\n",
    "WARMUP_PROPORTION = 0.1\n",
    "num_train_steps = int(len(training) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)+1\n",
    "NUM_WARMUP_STEPS =  int(num_train_steps * WARMUP_PROPORTION)\n",
    "RUN_NAME = \"test_trainer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9346f869",
   "metadata": {},
   "source": [
    "## Torch loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00ca5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetMC(nn.Module):\n",
    "    \"\"\"\n",
    "        Attach a FC layer on top of the BERT head in order to produce a classification output.\n",
    "\n",
    "        The pooled_output output of BERT is basically a projection of the [CLS] embeddings via another FC layer (768 -> 768 hidden units).\n",
    "        We stack another FC layer with Dropout on top of that, as reported in https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_classifier.py#L574\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels):\n",
    "        super(MyNetMC2, self).__init__()\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.model = copy.deepcopy(pretrained_model)#AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.linear1 = nn.Linear(768, 3)\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, labels, input_ids, attention_mask, **args):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **args)\n",
    "        x = self.dropout1(outputs[1])\n",
    "        logits = self.linear1(x)\n",
    "        loss = self.loss_fct(logits, labels)\n",
    "        return logits , loss\n",
    "    \n",
    "class MyNetMCTuned(nn.Module):\n",
    "    \"\"\"\n",
    "        Attach a FC layer on top of the BERT head in order to produce a classification output.\n",
    "\n",
    "        The pooled_output output of BERT is basically a projection of the [CLS] embeddings via another FC layer (768 -> 768 hidden units).\n",
    "        We stack another FC layer without Dropout on top of that, as reported in https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_classifier.py#L574\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels):\n",
    "        super(MyNetMC2, self).__init__()\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.model = copy.deepcopy(pretrained_model)\n",
    "        self.linear1 = nn.Linear(768, 3)\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, labels, input_ids, attention_mask, **args):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **args)\n",
    "        logits = self.linear(outputs[1])\n",
    "        loss = self.loss_fct(logits, labels)\n",
    "        return logits , loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccdd458",
   "metadata": {},
   "source": [
    "## Utility funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b4ddfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, epoch, logging):\n",
    "    model.train()\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    cumulative_loss = 0.\n",
    "    for i , data in tqdm(enumerate(train_loader, 0), total=len(train_loader)):        \n",
    "        targets.extend(data[\"labels\"].numpy())\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in data.items()}\n",
    "        logits , loss = model(**batch)\n",
    "\n",
    "        cumulative_loss += loss.detach()\n",
    "        if (i+1) % 25 == 0 and logging:\n",
    "            print(f'Epoch: {epoch}, Loss:  {cumulative_loss.item()/i}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        outputs.extend(logits.argmax(-1).cpu().detach().numpy().tolist())\n",
    "    #print(\"Cum_loss_opos = \", model.cum_loss_opos.item() / len(train_loader))\n",
    "    #print(\"Cum_loss_oneg = \", model.cum_loss_oneg.item() / len(train_loader))\n",
    "    #model.reset_cum_loss()\n",
    "    if logging: wandb.log({\"train\": {'loss': cumulative_loss.item() / len(outputs)}})\n",
    "    return outputs, targets\n",
    "    \n",
    "        \n",
    "def validation_epoch(model, epoch, val_loader, kind, logging):\n",
    "    model.eval()\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    cumulative_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(val_loader, 0):\n",
    "            #ids            = data['input_ids'].to(device, dtype = torch.long)\n",
    "            #mask           = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            #token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            #targets        = data['labels'].to(device, dtype = torch.long)\n",
    "            \n",
    "            batch = {k: v.to(device) for k, v in data.items()}\n",
    "            logits , loss = model(**batch)\n",
    "            cumulative_loss += loss.detach()\n",
    "            if (i+1) % 25 == 0 and logging:\n",
    "                print(f'Epoch: {epoch}, Loss:  {cumulative_loss.item()/i}')\n",
    "            \n",
    "            #outputs_opos , outputs_oneg , _ = model(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids, labels=targets)\n",
    "            \n",
    "            targets.extend(batch[\"labels\"].cpu().detach().numpy())\n",
    "            outputs.extend(logits.argmax(-1).cpu().detach().numpy().tolist())\n",
    "    if logging: wandb.log({kind: {'loss': cumulative_loss.item() / len(outputs)}})\n",
    "    return outputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a0d76",
   "metadata": {},
   "source": [
    "## HPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a51075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "def objective(trial):\n",
    "    model = MyNetMCTuned(3).to(device)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    lr = trial.suggest_float(\"learning_rate\", 2e-6, 2e-4, log=True)\n",
    "    wd = trial.suggest_float(\"weight_decay\", 1e-6, 1e-1)\n",
    "    warmup = trial.suggest_float(\"warmup_steps\", 0., 0.9, step=0.3),\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=lr,\n",
    "                              weight_decay=wd, \n",
    "                              eps=1e-6)\n",
    "\n",
    "    early_stopping = EarlyStopping(min_delta=0.005, patience=3)\n",
    "\n",
    "    train_loader = DataLoader(training.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"]), shuffle=True, batch_size=64)\n",
    "    val_loader = DataLoader(validating.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"]), batch_size=64)\n",
    "\n",
    "    num_epochs = 15\n",
    "    num_train_steps = int(len(train_loader) * num_epochs) + 1\n",
    "    NUM_WARMUP_STEPS =  int(num_train_steps * warmup[0])\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, NUM_WARMUP_STEPS, num_train_steps)\n",
    "\n",
    "    logging = False\n",
    "    \n",
    "    best_val = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        outputs, targets = train_epoch(model, train_loader, optimizer, scheduler, epoch, logging)\n",
    "        #f1_train = f1_score(targets, outputs, average='macro')\n",
    "        \n",
    "        outputs, targets = validation_epoch(model, epoch, val_loader, \"val\", logging)\n",
    "        f1_val = f1_score(targets, outputs, average='macro')\n",
    "        \n",
    "        if f1_val > best_val:\n",
    "            best_val = f1_val\n",
    "        if early_stopping.on_epoch_end(epoch, f1_val):\n",
    "            print(\"\\n\\nEARLY STOPPING AFTER {} EPOCHS\".format(epoch))\n",
    "            break\n",
    "\n",
    "        #print(f\"Val F1 = {f1_val}\")\n",
    "        trial.report(f1_val, epoch)\n",
    "        \n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "    return best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3d135d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyper = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30, timeout=600)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "best_hyper = trial.params\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16920ecb",
   "metadata": {},
   "source": [
    "## Training AlBERTo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "model = MyNetMC(3).to(device)\n",
    "\n",
    "if best_hyper is None:\n",
    "    best_hyper = {'learning_rate': TRAIN_BATCH_SIZE, 'warmup_steps': WARMUP_PROPORTION, 'weight_decay': WEIGHT_DECAY}\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=best_hyper[\"learning_rate\"],\n",
    "                              weight_decay=best_hyper[\"weight_decay\"], \n",
    "                              eps=1e-6)\n",
    "\n",
    "train_loader = DataLoader(training.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"]), shuffle=True, batch_size=32)\n",
    "val_loader = DataLoader(validating.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"]), batch_size=64)\n",
    "test_loader = DataLoader(testing.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"]), batch_size=64)\n",
    "\n",
    "num_epochs = NUM_EPOCHS\n",
    "num_train_steps = int(len(train_loader) * num_epochs) + 1\n",
    "NUM_WARMUP_STEPS =  int(num_train_steps * best_hyper[\"warmup_steps\"])\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, NUM_WARMUP_STEPS, num_train_steps)\n",
    "\n",
    "logging = True\n",
    "if logging:\n",
    "    run = wandb.init(\n",
    "      project='nlu_sentiment_analysis',\n",
    "      name=\"AlBERTo-MC\",\n",
    "      entity='mcstewe',\n",
    "      notes='',\n",
    "      reinit=True,\n",
    "      config=dict (\n",
    "        epochs=num_epochs\n",
    "      )\n",
    "    )\n",
    "    wandb.watch(model)\n",
    "\n",
    "best_value = 0\n",
    "for epoch in range(num_epochs):\n",
    "    outputs, targets = train_epoch(model, train_loader, optimizer, scheduler, epoch, logging)\n",
    "    f1_train = f1_score(targets, outputs, average='macro')\n",
    "    outputs, targets = validation_epoch(model, epoch, val_loader, \"val\", logging=False)\n",
    "    \n",
    "    f1_val = f1_score(targets, outputs, average='macro')\n",
    "    if f1_val > best_value:\n",
    "        best_value = f1_val\n",
    "        torch.save(model.state_dict(), \".data/models/alberto_multiclass.pt\")    \n",
    "    \n",
    "    print(f\"Val F1 = {f1_val}\")\n",
    "    outputs, targets = validation_epoch(model, epoch, test_loader, \"test\", logging=False)\n",
    "    f1_test = f1_score(targets, outputs, average='macro')\n",
    "    print(\"Test F1=\", f1_test)\n",
    "    if logging:\n",
    "        wandb.log({\"train\": {'f1': f1_train, \"lr\": optimizer.param_groups[0]['lr']}})\n",
    "        wandb.log({\"val\": {'f1': f1_val}})\n",
    "        wandb.log({\"test\": {'f1': f1_test}})\n",
    "    \n",
    "if logging:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8221bf79",
   "metadata": {},
   "source": [
    "## Training AlBERTo tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "model = MyNetMCTuned(3).to(device)\n",
    "\n",
    "if best_hyper is None:\n",
    "    best_hyper = {'learning_rate': 3.000003529363845e-06, 'warmup_steps': 0.6, 'weight_decay': 0.000260393798851559}\n",
    "\n",
    "\n",
    "# we will iterate through the layers of the network\n",
    "#final_weights = []\n",
    "#pretrained_weights = []\n",
    "# for name, param in model.named_parameters():\n",
    "#     if name.startswith('model'):\n",
    "#         pretrained_weights.append(param)\n",
    "#     else:\n",
    "#         final_weights.append(param)\n",
    "#optimizer = torch.optim.AdamW(params=[\n",
    "#                                {'params': pretrained_weights, 'lr': 1e-5},\n",
    "#                                {'params': final_weights, 'lr': 6e-5}],\n",
    "#                              weight_decay=0.01, \n",
    "#                              eps=1e-6)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=best_hyper[\"learning_rate\"],\n",
    "                              weight_decay=best_hyper[\"weight_decay\"], \n",
    "                              eps=1e-6)\n",
    "early_stopping = EarlyStopping(min_delta=0.005, patience=3)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(training.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"]), shuffle=True, batch_size=32)\n",
    "val_loader = DataLoader(validating.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"]), batch_size=64)\n",
    "test_loader = DataLoader(testing.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"]), batch_size=64)\n",
    "\n",
    "num_epochs = 15\n",
    "num_train_steps = int(len(train_loader) * num_epochs) + 1\n",
    "NUM_WARMUP_STEPS =  int(num_train_steps * best_hyper[\"warmup_steps\"])\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, NUM_WARMUP_STEPS, num_train_steps)\n",
    "\n",
    "logging = True\n",
    "if logging:\n",
    "    run = wandb.init(\n",
    "      project='nlu_sentiment_analysis',\n",
    "      name=\"AlBERTo-MC-tuned\",\n",
    "      entity='mcstewe',\n",
    "      notes='',\n",
    "      reinit=True,\n",
    "      config=dict (\n",
    "        epochs=num_epochs\n",
    "      )\n",
    "    )\n",
    "    wandb.watch(model)\n",
    "\n",
    "best_value = 0\n",
    "for epoch in range(num_epochs):\n",
    "    outputs, targets = train_epoch(model, train_loader, optimizer, scheduler, epoch, logging)\n",
    "    f1_train = f1_score(targets, outputs, average='macro')\n",
    "    outputs, targets = validation_epoch(model, epoch, val_loader, \"val\", logging=False)\n",
    "    \n",
    "    f1_val = f1_score(targets, outputs, average='macro')\n",
    "    if f1_val > best_value:\n",
    "        best_value = f1_val\n",
    "        torch.save(model.state_dict(), \".data/models/alberto_multiclass_tuned.pt\")    \n",
    "    if early_stopping.on_epoch_end(epoch, f1_val):\n",
    "        print(\"\\n\\nEARLY STOPPING AFTER {} EPOCHS\".format(epoch))\n",
    "        break\n",
    "    \n",
    "    print(f\"Val F1 = {f1_val}\")\n",
    "    outputs, targets = validation_epoch(model, epoch, test_loader, \"test\", logging=False)\n",
    "    f1_test = f1_score(targets, outputs, average='macro')\n",
    "    print(\"Test F1=\", f1_test)\n",
    "    if logging:\n",
    "        wandb.log({\"train\": {'f1': f1_train, \"lr\": optimizer.param_groups[0]['lr']}})\n",
    "        wandb.log({\"val\": {'f1': f1_val}})\n",
    "        wandb.log({\"test\": {'f1': f1_test}})\n",
    "    \n",
    "if logging:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44600230",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc2d20",
   "metadata": {},
   "source": [
    "### AlBERTo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best model's params\n",
    "model = MyNetMC(3).to(device)\n",
    "model.load_state_dict(torch.load(\".data/models/alberto_multiclass.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65edaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(testing.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"]), batch_size=64)\n",
    "preds , trues = validation_epoch(model, None, test_loader, \"test\", logging=False)\n",
    "\n",
    "dataset_test = pd.DataFrame({\"text\": test.text.apply(a.preprocess), \"idx\": test.index, \"labels\": test[[\"opos\", \"oneg\"]].apply(separate2united_labels, axis=1)})\n",
    "dataset_test = dataset_test[dataset_test.labels != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4966e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n , (i , row) in enumerate(dataset_test.iterrows()):\n",
    "    if preds[n] != trues[n]:\n",
    "        print(f\"{row['text']} ------ true={row['labels']} pred={preds[n]}  \\n\")\n",
    "        assert row['labels'] == trues[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2184ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(trues, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "cm = confusion_matrix(trues, preds, normalize=True)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"negative\", \"neutral\", \"positive\"]).plot(values_format=\"d\")\n",
    "plt.title(\"Test AlBERToMC - raw tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7c3190",
   "metadata": {},
   "source": [
    "### AlBERTo tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best model's params\n",
    "model = MyNetMCTuned(3).to(device)\n",
    "model.load_state_dict(torch.load(\".data/models/alberto_multiclass_tuned.pt\"))\n",
    "\n",
    "preds , trues = validation_epoch(model, None, test_loader, \"test\", logging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n , (i , row) in enumerate(dataset_test.iterrows()):\n",
    "    if preds[n] != trues[n]:\n",
    "        print(f\"{row['text']} ------ true={row['labels']} pred={preds[n]}  \\n\")\n",
    "        assert row['labels'] == trues[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db8525",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(trues, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "cm = confusion_matrix(trues, preds, normalize=True)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"negative\", \"neutral\", \"positive\"]).plot(values_format=\"d\")\n",
    "plt.title(\"Test AlBERToMC tuned - raw tweets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
