{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AlBERTo reproduction.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1LsSHoGyEBmpjcFOE9rzurxuK2qqviDTI","authorship_tag":"ABX9TyPttUE8v2Kfr2TV3BqDpset"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d20b82ff1c284b1484f80c668cb5e98d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b17b11dc49eb479e9e08bfcf0cbd7fd9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9059cc6960064f3694efc13301a0b401","IPY_MODEL_a33f259868d1495ca114653e7931ac32"]}},"b17b11dc49eb479e9e08bfcf0cbd7fd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9059cc6960064f3694efc13301a0b401":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b74be79ed20744a3816a311dfb3c8d3f","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":8,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":8,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c2fab4e77c244aeb99b23b24b6b9c4eb"}},"a33f259868d1495ca114653e7931ac32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5434d8840dd447f2a6bdd50a8c942855","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 8/8 [00:01&lt;00:00,  6.76ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_abb58cba28fd43b68cf608854a94bffb"}},"b74be79ed20744a3816a311dfb3c8d3f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c2fab4e77c244aeb99b23b24b6b9c4eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5434d8840dd447f2a6bdd50a8c942855":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"abb58cba28fd43b68cf608854a94bffb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"82fdd6cffea840d68c3116d739ddda10":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_85fcd5e1d4964c818a20fa3179112ac5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ffc4c651c7984a709e505bf86e68368f","IPY_MODEL_984519c0accd42ed9d1e04cc6e2fccf8"]}},"85fcd5e1d4964c818a20fa3179112ac5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ffc4c651c7984a709e505bf86e68368f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9139c3076bf043bbae2603df33f11b14","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a002ef9b7d0b4d1c995129a1dc398957"}},"984519c0accd42ed9d1e04cc6e2fccf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5af535186cc249a7baa65f6458a1b418","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:00&lt;00:00,  8.98ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_efe0a496168747329bc69e6ebdacc118"}},"9139c3076bf043bbae2603df33f11b14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a002ef9b7d0b4d1c995129a1dc398957":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5af535186cc249a7baa65f6458a1b418":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"efe0a496168747329bc69e6ebdacc118":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"691146737a01402aa0d63f98d438ad7c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cfdba240125548589b69427a6871a80f","IPY_MODEL_720eb2f44a9c462a839509eaa1991026"],"layout":"IPY_MODEL_d6067f76840a45748c12a48723207c32"}},"cfdba240125548589b69427a6871a80f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"  1%","description_tooltip":null,"layout":"IPY_MODEL_090e409276454d40aacf2010a8fcc503","max":2781,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e99c5afce9c74bd1a906275df4dcf50d","value":40}},"720eb2f44a9c462a839509eaa1991026":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4029161b53d2477cbe24ef2a5fd4a3be","placeholder":"​","style":"IPY_MODEL_540c54658aef4c5780024c1ebfc88e59","value":" 40/2781 [00:32&lt;38:13,  1.19it/s]"}},"d6067f76840a45748c12a48723207c32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"090e409276454d40aacf2010a8fcc503":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e99c5afce9c74bd1a906275df4dcf50d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"4029161b53d2477cbe24ef2a5fd4a3be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"540c54658aef4c5780024c1ebfc88e59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hHOn2JLh2Ezo","executionInfo":{"status":"ok","timestamp":1625430642462,"user_tz":-120,"elapsed":3913,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}},"outputId":"fad793d6-034f-411d-b3ab-17f426b70c33"},"source":["!pip install ekphrasis"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ekphrasis in /usr/local/lib/python3.7/dist-packages (0.5.1)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (0.4.4)\n","Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.19.5)\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (6.0.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.41.1)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.3.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQmgjCXANta3","executionInfo":{"status":"ok","timestamp":1625430642462,"user_tz":-120,"elapsed":16,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}},"outputId":"4f923e30-6343-48f2-dd02-d9558764db58"},"source":["%env WANDB_PROJECT=nlu_sentiment_analysis\n","import wandb\n","wandb.login()"],"execution_count":19,"outputs":[{"output_type":"stream","text":["env: WANDB_PROJECT=nlu_sentiment_analysis\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"F_m8kAWN0bFl","executionInfo":{"status":"ok","timestamp":1625430642463,"user_tz":-120,"elapsed":12,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}}},"source":["from ekphrasis.classes.preprocessor import TextPreProcessor\n","from ekphrasis.classes.tokenizer import SocialTokenizer\n","from ekphrasis.dicts.emoticons import emoticons\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","import itertools\n","import requests, re, string, datetime\n","\n","PATH = \"/content/drive/MyDrive/Colab Notebooks/NLU/NLU Project/\""],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"PYinAa3s0xvy","executionInfo":{"status":"ok","timestamp":1625430642464,"user_tz":-120,"elapsed":12,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}},"outputId":"63973dbe-c7ba-4b33-c9e7-88928f3c397d"},"source":["train = pd.read_csv(PATH + \"data/Sentipolc16/training_set_sentipolc16.csv\")\n","train.head()"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idtwitter</th>\n","      <th>subj</th>\n","      <th>opos</th>\n","      <th>oneg</th>\n","      <th>iro</th>\n","      <th>lpos</th>\n","      <th>lneg</th>\n","      <th>top</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>122449983151669248</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Intanto la partita per Via Nazionale si compli...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>125485104863780865</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>False illusioni, sgradevoli realtà Mario Monti...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>125513454315507712</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>False illusioni, sgradevoli realtà #editoriale...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>125524238290522113</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Mario Monti: Berlusconi risparmi all'Italia il...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>125527933224886272</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Mario Monti: Berlusconi risparmi all'Italia il...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            idtwitter  ...                                               text\n","0  122449983151669248  ...  Intanto la partita per Via Nazionale si compli...\n","1  125485104863780865  ...  False illusioni, sgradevoli realtà Mario Monti...\n","2  125513454315507712  ...  False illusioni, sgradevoli realtà #editoriale...\n","3  125524238290522113  ...  Mario Monti: Berlusconi risparmi all'Italia il...\n","4  125527933224886272  ...  Mario Monti: Berlusconi risparmi all'Italia il...\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":203},"id":"lIddrEdl0zK6","executionInfo":{"status":"ok","timestamp":1625430642464,"user_tz":-120,"elapsed":10,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}},"outputId":"88ae7a4e-3653-41de-8590-21033ccd875c"},"source":["file1 = open(PATH + \"data/Sentipolc16/test_set_sentipolc16_gold2000.csv\", 'r') \n","Lines = file1.readlines()\n"," \n","test = []\n","for line in Lines:\n","  arr = line.split(\"\\\",\")\n","  if len(arr) != 9:\n","    arr[8] = arr[8] + arr[9]  #to account for tweets containing the delimiter charachter that would create more splits than needed\n","    del arr[9:]\n","  for i in range(8):\n","    arr[i] = int(arr[i].strip(\"\\\"\"))\n","  test.append(arr)\n","\n","test = pd.DataFrame(test, columns=train.columns)\n","test.head()"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>idtwitter</th>\n","      <th>subj</th>\n","      <th>opos</th>\n","      <th>oneg</th>\n","      <th>iro</th>\n","      <th>lpos</th>\n","      <th>lneg</th>\n","      <th>top</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>507074506880712705</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>\"Tra 5 minuti presentazione piano scuola del g...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>507075789456961536</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>\"\\\"@matteorenzi: Alle 10 appuntamento su http:...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>507077511902425088</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>\"#labuonascuola gli #evangelisti #digitali non...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>507079183315787777</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>\"Riforma scuola Tutto il discorso di  Renzi su...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>507080190225563648</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>\".@matteorenzi @MiurSocial #labuonascuola bast...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            idtwitter  ...                                               text\n","0  507074506880712705  ...  \"Tra 5 minuti presentazione piano scuola del g...\n","1  507075789456961536  ...  \"\\\"@matteorenzi: Alle 10 appuntamento su http:...\n","2  507077511902425088  ...  \"#labuonascuola gli #evangelisti #digitali non...\n","3  507079183315787777  ...  \"Riforma scuola Tutto il discorso di  Renzi su...\n","4  507080190225563648  ...  \".@matteorenzi @MiurSocial #labuonascuola bast...\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KzI1KteE3_8i","executionInfo":{"status":"ok","timestamp":1625430661296,"user_tz":-120,"elapsed":18840,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}},"outputId":"6237b84f-179d-46ee-bb6f-5b476c475ab1"},"source":["text_processor = TextPreProcessor (\n","    # terms that will be normalized\n","    normalize=[ 'url' , 'email', 'user', 'percent', 'money', 'phone', 'time', 'date', 'number'] ,\n","    # terms that will be annotated\n","    annotate={\"hashtag\"} ,\n","    fix_html=True ,  # fix HTML tokens\n","\n","    unpack_hashtags=True ,  # perform word segmentation on hashtags\n","\n","    # select a tokenizer. You can use SocialTokenizer, or pass your own\n","    # the tokenizer, should take as input a string and return a list of tokens\n","    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n","    dicts = [ emoticons ]\n",")\n","\n","sentences = train[\"text\"]\n","labels = train.iloc[:,1]\n","\n","examples = []\n","for i , s in enumerate(sentences):\n","    s = s.lower()\n","    s = str(\" \".join(text_processor.pre_process_doc(s)))\n","    s = re.sub(r\"[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]\", ' ', s)\n","    s = re.sub(r\"\\s+\", ' ', s)\n","    s = re.sub(r'(\\w)\\1{2,}',r'\\1\\1', s)\n","    s = re.sub ( r'^\\s' , '' , s )\n","    s = re.sub ( r'\\s$' , '' , s )\n","    examples.append([labels[i],s])\n","    i = i+1\n","examples = np.array(examples)\n","\n","\n","sentences_test = test[\"text\"]\n","test_ids = list(test.index)\n","\n","examples_test = []\n","for i , s in enumerate(sentences_test):\n","    s = s.lower()\n","    s = str(\" \".join(text_processor.pre_process_doc(s)))\n","    s = re.sub(r\"[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]\", ' ', s)\n","    s = re.sub(r\"\\s+\", ' ', s)\n","    s = re.sub(r'(\\w)\\1{2,}',r'\\1\\1', s)\n","    s = re.sub ( r'^\\s' , '' , s )\n","    s = re.sub ( r'\\s$' , '' , s )\n","    examples_test.append([labels[i],s])\n","    i = i+1\n","examples_test = np.array(examples_test)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Reading english - 1grams ...\n","Reading english - 2grams ...\n","Reading english - 1grams ...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VpDIUkBNgFM-"},"source":["#### tokenizer.py"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfsLJ5GGg37A","executionInfo":{"status":"ok","timestamp":1625430669787,"user_tz":-120,"elapsed":8496,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}},"outputId":"8096f0a2-efb2-43cf-964c-200bef3100f9"},"source":["!pip install tqdm boto3 requests regex sentencepiece sacremoses pytorch-transformers\n","!pip install transformers\n","!pip install datasets"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.17.105)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (0.0.45)\n","Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.7/dist-packages (1.2.0)\n","Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.4.2)\n","Requirement already satisfied: botocore<1.21.0,>=1.20.105 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.20.105)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.10.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.9.0+cu102)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.19.5)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.105->boto3) (2.8.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.8.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.6.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.12)\n","Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AlEMxwOAgG4H","executionInfo":{"status":"ok","timestamp":1625430679314,"user_tz":-120,"elapsed":9542,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}},"outputId":"d7e0d0a1-b8ca-4c5f-c2ee-dc52fda6dc5a"},"source":["\"\"\"Tokenization classes for Italian AlBERTo models.\"\"\"\n","import collections\n","import logging\n","import os\n","import re\n","\n","from ekphrasis.classes.preprocessor import TextPreProcessor\n","from ekphrasis.classes.tokenizer import SocialTokenizer\n","from ekphrasis.dicts.emoticons import emoticons\n","\n","import numpy as np\n","\n","from transformers import BertTokenizer, WordpieceTokenizer\n","\n","def separate2united_labels(row):\n","  \"\"\"\n","  Return a single scalar integer label associated to the polarity of the tweet.\n","\n","  Negative -> 0\n","  Neutral  -> 1\n","  Positive -> 2\n","  Mixed    -> 3\n","  \"\"\"\n","  if row[\"opos\"] == 0 and row[\"oneg\"] == 0:\n","    return 1\n","  elif row[\"oneg\"] == 0 and row[\"opos\"] == 1:\n","    return 2\n","  elif row[\"oneg\"] == 1 and row[\"opos\"] == 0:\n","    return 0\n","  else:\n","    return 3\n","\n","def load_vocab(vocab_file):\n","    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n","    vocab = collections.OrderedDict()\n","    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n","        tokens = reader.readlines()\n","    for index, token in enumerate(tokens):\n","        token = token.rstrip(\"\\n\")\n","        vocab[token] = index\n","    return vocab\n","\n","text_processor = TextPreProcessor(\n","    # terms that will be normalized\n","    normalize=['url', 'email', 'user', 'percent', 'money', 'phone', 'time', 'date', 'number'],\n","    # terms that will be annotated\n","    annotate={\"hashtag\"},\n","    fix_html=True,  # fix HTML tokens\n","\n","    unpack_hashtags=True,  # perform word segmentation on hashtags\n","\n","    # select a tokenizer. You can use SocialTokenizer, or pass your own\n","    # the tokenizer, should take as input a string and return a list of tokens\n","    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n","    dicts=[emoticons]\n",")\n","\n","class AlBERTo_Preprocessing(object):\n","    def __init__(self, do_lower_case=True, **kwargs):\n","        self.do_lower_case = do_lower_case\n","\n","    def preprocess(self, text):\n","        if self.do_lower_case:\n","            text = text.lower()\n","        text = str(\" \".join(text_processor.pre_process_doc(text)))\n","        text = re.sub(r'[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]', ' ', text)\n","        text = re.sub(r'\\s+', ' ', text)\n","        text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n","        text = re.sub(r'^\\s', '', text)\n","        text = re.sub(r'\\s$', '', text)\n","        return text\n","\n","class AlBERToTokenizer(BertTokenizer):\n","\n","    def __init__(self, vocab_file, do_lower_case=True,\n","                 do_basic_tokenize=True, do_char_tokenize=False, do_wordpiece_tokenize=False, do_preprocessing = True, unk_token='[UNK]',\n","                 sep_token='[SEP]',\n","                 pad_token='[PAD]', cls_token='[CLS]', mask_token='[MASK]', **kwargs):\n","        super(BertTokenizer, self).__init__(\n","            unk_token=unk_token, sep_token=sep_token, pad_token=pad_token,\n","            cls_token=cls_token, mask_token=mask_token, **kwargs)\n","\n","        self.do_wordpiece_tokenize = do_wordpiece_tokenize\n","        self.lower_case = do_lower_case\n","        self.vocab_file = vocab_file\n","        self.do_basic_tokenize = do_basic_tokenize\n","        self.do_char_tokenize = do_char_tokenize\n","        self.unk_token = unk_token\n","        self.do_preprocessing = do_preprocessing\n","\n","        if not os.path.isfile(vocab_file):\n","            raise ValueError(\n","                \"Can't find a vocabulary file at path '{}'.\".format(vocab_file))\n","\n","        self.vocab = load_vocab(vocab_file)\n","        self.ids_to_tokens = collections.OrderedDict(\n","            [(ids, tok) for tok, ids in self.vocab.items()])\n","\n","        if do_wordpiece_tokenize:\n","            self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab,\n","                                                          unk_token=self.unk_token)\n","            \n","        self.base_bert_tok = BertTokenizer(vocab_file=self.vocab_file, do_lower_case=do_lower_case,\n","                                      unk_token=unk_token, sep_token=sep_token, pad_token=pad_token,\n","                                      cls_token=cls_token, mask_token=mask_token, **kwargs)\n","\n","    def _convert_token_to_id(self, token):\n","        \"\"\"Converts a token (str/unicode) to an id using the vocab.\"\"\"\n","        # if token[:2] == '##':\n","        #     token = token[2:]\n","\n","        return self.vocab.get(token, self.vocab.get(self.unk_token))\n","\n","    def convert_token_to_id(self, token):\n","        return self._convert_token_to_id(token)\n","\n","        return self.vocab.get(token, self.vocab.get(self.unk_token))\n","\n","    def _convert_id_to_token(self, id):\n","        # if token[:2] == '##':\n","        #     token = token[2:]\n","\n","        return list(self.vocab.keys())[int(id)]\n","    def convert_id_to_token(self, id):\n","        return self._convert_id_to_token(id)\n","\n","    def _convert_tokens_to_string(self,tokens):\n","        \"\"\"Converts a sequence of tokens (string) to a single string.\"\"\"\n","        out_string = ' '.join(tokens).replace('##', '').strip()\n","        return out_string\n","\n","    def convert_tokens_to_string(self,tokens):\n","        return self._convert_tokens_to_string(tokens)\n","\n","    def _tokenize(self, text, never_split=None, **kwargs):\n","        if self.do_preprocessing:\n","            if self.lower_case:\n","                text = text.lower()\n","            text = str(\" \".join(text_processor.pre_process_doc(text)))\n","            text = re.sub(r'[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]', ' ', text)\n","            text = re.sub(r'\\s+', ' ', text)\n","            text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n","            text = re.sub(r'^\\s', '', text)\n","            text = re.sub(r'\\s$', '', text)\n","            # print(s)\n","\n","        split_tokens = [text]\n","        if self.do_wordpiece_tokenize:\n","            wordpiece_tokenizer = WordpieceTokenizer(self.vocab,self.unk_token)\n","            split_tokens = wordpiece_tokenizer.tokenize(text)\n","\n","        elif self.do_char_tokenize:\n","            tokenizer = CharacterTokenizer(self.vocab, self.unk_token)\n","            split_tokens = tokenizer.tokenize(text)\n","\n","        elif self.do_basic_tokenize:\n","            \"\"\"Tokenizes a piece of text.\"\"\"\n","            split_tokens = self.base_bert_tok.tokenize(text)\n","\n","        return split_tokens\n","\n","    def tokenize(self, text, never_split=None, **kwargs):\n","        return self._tokenize(text, never_split)\n","\n","\n","class CharacterTokenizer(object):\n","    \"\"\"Runs Character tokenziation.\"\"\"\n","\n","    def __init__(self, vocab, unk_token,\n","                 max_input_chars_per_word=100, with_markers=True):\n","        \"\"\"Constructs a CharacterTokenizer.\n","        Args:\n","            vocab: Vocabulary object.\n","            unk_token: A special symbol for out-of-vocabulary token.\n","            with_markers: If True, \"#\" is appended to each output character except the\n","                first one.\n","        \"\"\"\n","        self.vocab = vocab\n","        self.unk_token = unk_token\n","        self.max_input_chars_per_word = max_input_chars_per_word\n","        self.with_markers = with_markers\n","\n","    def tokenize(self, text):\n","        \"\"\"Tokenizes a piece of text into characters.\n","\n","        For example:\n","            input = \"apple\"\n","            output = [\"a\", \"##p\", \"##p\", \"##l\", \"##e\"]  (if self.with_markers is True)\n","            output = [\"a\", \"p\", \"p\", \"l\", \"e\"]          (if self.with_markers is False)\n","        Args:\n","            text: A single token or whitespace separated tokens.\n","                This should have already been passed through `BasicTokenizer`.\n","        Returns:\n","            A list of characters.\n","        \"\"\"\n","\n","        output_tokens = []\n","        for i, char in enumerate(text):\n","            if char not in self.vocab:\n","                output_tokens.append(self.unk_token)\n","                continue\n","\n","            if self.with_markers and i != 0:\n","                output_tokens.append('##' + char)\n","            else:\n","                output_tokens.append(char)\n","\n","        return output_tokens\n","\n","a = AlBERTo_Preprocessing(do_lower_case=True)\n","s = \"#IlGOverno presenta le linee guida sulla scuola #labuonascuola - http://t.co/SYS1T9QmQN\"\n","b = a.preprocess(s)\n","print(b)\n","\n","#c = AlBERToTokenizer(do_lower_case=True,vocab_file=VOCAB_FILE, do_preprocessing=True)\n","#d = c.tokenize(s)\n","#print(d)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Reading english - 1grams ...\n","Reading english - 2grams ...\n","Reading english - 1grams ...\n","<hashtag> il governo </hashtag> presenta le linee guida sulla scuola <hashtag> la buona scuola </hashtag> <url>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A6NFBdibdH2g"},"source":["#### Pytorch"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vydTF7czEA1f","executionInfo":{"status":"ok","timestamp":1625430705444,"user_tz":-120,"elapsed":26132,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}},"outputId":"47d40d0b-ed63-4015-dbe4-546def6df9ef"},"source":["from transformers import AutoTokenizer, AutoModel\n","\n","a = AlBERTo_Preprocessing(do_lower_case=True)\n","s: str = \"#IlGOverno presenta le linee guida sulla scuola #labuonascuola - http://t.co/SYS1T9QmQN\"\n","b = a.preprocess(s)\n","\n","tok = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n","model = AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n","\n","tok.model_max_length = 128 #model.config.max_position_embeddings\n","tokens = tok.tokenize(b)\n","print(tokens)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ddbe3a15a091d937ecbe95bd9c6fc3c82a8c0fde2b2725504806bae3d5b553c7.ae4dca69541bd7de0c171dd17916268b75b9de1cdbd9670add7a62ad5b98a01e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.8.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 128000\n","}\n","\n","loading file https://huggingface.co/m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/174b6e5587731bd9037ef8df62f237a8d81fd01eda90377bd9ed2b85398924db.482ac9adad355c8c6ca8a06659cbd458c5b9b1c5f99bcc481a81417d03c4b21c\n","loading file https://huggingface.co/m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ddbe3a15a091d937ecbe95bd9c6fc3c82a8c0fde2b2725504806bae3d5b553c7.ae4dca69541bd7de0c171dd17916268b75b9de1cdbd9670add7a62ad5b98a01e\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.8.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 128000\n","}\n","\n","loading weights file https://huggingface.co/m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8270b211710cba9f85a4fd7242e31b37d94e76042d67de09b6eac7ab6e7ef78e.48dbff3c9ce886c188677656cbfb17029a8a066539b2da658b60d42766b0122c\n","Some weights of the model checkpoint at m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of BertModel were initialized from the model checkpoint at m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["['<', 'ha', '##shtag', '>', 'il', 'governo', '<', '/', 'ha', '##shtag', '>', 'presenta', 'le', 'linee', 'guida', 'sulla', 'scuola', '<', 'ha', '##shtag', '>', 'la', 'buona', 'scuola', '<', '/', 'ha', '##shtag', '>', '<', 'ur', '##l', '>']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhqNIs3RGnTd","executionInfo":{"status":"ok","timestamp":1625430705786,"user_tz":-120,"elapsed":345,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}},"outputId":"8ce7833d-f961-4c72-b1ff-f2a95980655b"},"source":["output = model(**tok(\"ciao \" * 1000, return_tensors=\"pt\", truncation=True))\n","\n","output[0][:,0,:].shape , output[1].shape , output[0][:,0,:10] , output[1][0][:10]"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 768]),\n"," torch.Size([1, 768]),\n"," tensor([[-0.6951, -0.8461, -0.6175, -0.8552,  0.0815,  1.5553,  0.7703, -0.3757,\n","           0.9055,  1.7895]], grad_fn=<SliceBackward>),\n"," tensor([-0.6683,  0.9256, -0.5889, -0.0618,  0.6870,  0.0903, -0.0759,  0.7948,\n","         -0.5468,  0.2115], grad_fn=<SliceBackward>))"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":151,"referenced_widgets":["d20b82ff1c284b1484f80c668cb5e98d","b17b11dc49eb479e9e08bfcf0cbd7fd9","9059cc6960064f3694efc13301a0b401","a33f259868d1495ca114653e7931ac32","b74be79ed20744a3816a311dfb3c8d3f","c2fab4e77c244aeb99b23b24b6b9c4eb","5434d8840dd447f2a6bdd50a8c942855","abb58cba28fd43b68cf608854a94bffb","82fdd6cffea840d68c3116d739ddda10","85fcd5e1d4964c818a20fa3179112ac5","ffc4c651c7984a709e505bf86e68368f","984519c0accd42ed9d1e04cc6e2fccf8","9139c3076bf043bbae2603df33f11b14","a002ef9b7d0b4d1c995129a1dc398957","5af535186cc249a7baa65f6458a1b418","efe0a496168747329bc69e6ebdacc118"]},"id":"4ThsIe1zM8tT","executionInfo":{"status":"ok","timestamp":1625430712179,"user_tz":-120,"elapsed":6397,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}},"outputId":"bd0d7ef1-67fb-43f7-f105-80ea6a5a868b"},"source":["## Dataset\n","from datasets import Dataset\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","\n","def tokenize_function(examples):\n","    sa = tok(examples[\"text\"], padding=\"max_length\", truncation=True)\n","    return sa\n","\n","df = pd.DataFrame({\"text\": train.text.apply(a.preprocess), \"idx\": train.index, \"labels\": train[[\"opos\", \"oneg\"]].apply(separate2united_labels, axis=1)})\n","dataset = Dataset.from_pandas(df)\n","print(dataset[0])\n","\n","training = dataset.map(tokenize_function, batched=True).shuffle(seed=42).with_format(\"torch\")\n","\n","df = pd.DataFrame({\"text\": test.text.apply(a.preprocess), \"idx\": test.index, \"labels\": test[[\"opos\", \"oneg\"]].apply(separate2united_labels, axis=1)})\n","dataset = Dataset.from_pandas(df)\n","\n","testing = dataset.map(tokenize_function, batched=True).with_format(\"torch\")\n","\n","\n","# for s in training[0].keys():\n","#   if type(training[0][s]) != list:\n","#     print(s, training[0][s])\n","#   else:\n","#     print(s, \"len=\" ,len(training[0][s]))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["{'text': 'intanto la partita per via nazionale si complica <hashtag> sacco manni </hashtag> dice che mica tutti sono mario <hashtag> monti </hashtag> <url> via <user>', 'idx': 0, 'labels': 0}\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d20b82ff1c284b1484f80c668cb5e98d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82fdd6cffea840d68c3116d739ddda10","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UxyFFIo9qbaZ","executionInfo":{"status":"ok","timestamp":1625430712179,"user_tz":-120,"elapsed":4,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}}},"source":["# print(torch.cuda.get_device_name(0))\n","# print('Memory Usage:')\n","# print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","# print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MlLFm-gwuohk","executionInfo":{"status":"ok","timestamp":1625430713811,"user_tz":-120,"elapsed":1635,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}},"outputId":"feceaf86-b29b-41ab-d7f3-e21d282fe939"},"source":["training[\"token_type_ids\"].element_size() * training[\"token_type_ids\"].nelement() / 1000000 #, training[\"token_type_ids\"].type(torch.int8) "],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7.58784"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"71slkjOjhSI3","executionInfo":{"status":"ok","timestamp":1625430713812,"user_tz":-120,"elapsed":7,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}}},"source":["## Model\n","import torch\n","import torchvision.transforms as T, torch.nn.functional as F, torch.nn as nn\n","\n","class MyNet(nn.Module):\n","  \"\"\"\n","    Attach a FC layer on top of the BERT head in order to produce a classification output.\n","    Hyperparameters are taken from Alberto.\n","\n","    The pooled_output output of BERT is basically a projection of the [CLS] embeddings via another FC layer (768 -> 768 hidden units).\n","    We stack another FC layer with Dropout on top of that, as reported in https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_classifier.py#L574\n","  \"\"\"\n","\n","  def __init__(self):\n","    super(MyNet, self).__init__()\n","\n","    self.model = AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n","    self.dropout = nn.Dropout(0.1)\n","    self.linear = nn.Linear(768, 4)\n","\n","    self.loss_fct = nn.CrossEntropyLoss()\n","\n","\n","  def forward(self, labels, input_ids, **args):\n","    #For the output format -> https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForSequenceClassification.forward\n","    outputs = self.model(input_ids, **args)\n","    x = self.dropout(outputs[1])\n","    logits = self.linear(x)\n","    \n","    loss = self.loss_fct(logits, labels)\n","  \n","    return SequenceClassifierOutput(\n","        loss=loss,\n","        logits=logits,\n","        hidden_states=outputs.hidden_states,\n","        attentions=outputs.attentions,\n","    )"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5BzjqIXeTr1Q"},"source":["#### Trainer loop"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":229},"id":"sPbizIX_PvVK","executionInfo":{"status":"error","timestamp":1625435146348,"user_tz":-120,"elapsed":1296,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"}},"outputId":"415c637b-0341-4030-a8d0-46c106be786a"},"source":["from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","\n","\n","TRAIN_BATCH_SIZE = 32 #512\n","PREDICT_BATCH_SIZE = 32 #512\n","EVAL_BATCH_SIZE = 32 #512\n","LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 3 #10\n","MAX_SEQ_LENGTH = 128\n","\n","# Warmup is a period of time where hte learning rate \n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","num_train_steps = int(len(training) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)+1\n","NUM_WARMUP_STEPS = int(NUM_TRAIN_EPOCHS * WARMUP_PROPORTION)\n","\n","training_args = TrainingArguments(\"test_trainer\", \n","                                  num_train_epochs=3,\n","                                  per_device_train_batch_size=32,\n","                                  per_device_eval_batch_size=32,\n","                                  evaluation_strategy=\"epoch\",\n","                                  learning_rate=LEARNING_RATE,\n","                                  warmup_steps=NUM_WARMUP_STEPS,\n","                                  weight_decay=0.01,\n","                                  adam_beta1=0.9,\n","                                  adam_beta2=0.999,\n","                                  adam_epsilon=1e-6,\n","                                  report_to=\"wandb\",  \n","                                  run_name=\"alberto-repr-colab-2\"\n",")\n","\n","np.random.seed(0)\n","torch.manual_seed(0)\n","\n","trainer = Trainer(\n","    model_init=MyNet, \n","    args=training_args, \n","    train_dataset=training , \n","    eval_dataset=testing,\n","    compute_metrics=compute_metrics\n",")\n","\n","\n","trainer.train()\n","wandb.finish()"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ecf968fdb64e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# is small and gradually increases--usually helps training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mWARMUP_PROPORTION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mnum_train_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mTRAIN_BATCH_SIZE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mNUM_TRAIN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mNUM_WARMUP_STEPS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_TRAIN_EPOCHS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mWARMUP_PROPORTION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'training' is not defined"]}]},{"cell_type":"code","metadata":{"id":"EK-34d-qwZzE"},"source":["1\tNo log\t0.825890\t0.681000\t0.488477\t0.503568\t0.499365\n","2\tNo log\t0.897935\t0.657500\t0.491635\t0.553197\t0.489481\n","3\t0.843400\t0.975949\t0.645000\t0.494844\t0.524969\t0.503557"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yn6CwTd5TnkR"},"source":["#### explicit training loop"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":473,"referenced_widgets":["691146737a01402aa0d63f98d438ad7c","cfdba240125548589b69427a6871a80f","720eb2f44a9c462a839509eaa1991026","d6067f76840a45748c12a48723207c32","090e409276454d40aacf2010a8fcc503","e99c5afce9c74bd1a906275df4dcf50d","4029161b53d2477cbe24ef2a5fd4a3be","540c54658aef4c5780024c1ebfc88e59"]},"id":"cNfsSOIyOELZ","executionInfo":{"elapsed":36482,"status":"error","timestamp":1625333923712,"user":{"displayName":"Steve Azzolin","photoUrl":"","userId":"05451306045876431718"},"user_tz":-120},"outputId":"b1af6e48-85f1-43b3-f97e-8437369c3feb"},"source":["from torch.utils.data import DataLoader\n","\n","tokenized_datasets = training.remove_columns([\"text\", \"idx\"])\n","tokenized_datasets.set_format(\"torch\")\n","train_dataloader = DataLoader(tokenized_datasets, shuffle=True, batch_size=8)\n","\n","from transformers import AdamW\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","from transformers import get_scheduler\n","\n","num_epochs = 3\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps\n",")\n","\n","import torch\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model=MyNet()\n","model.to(device)\n","\n","from tqdm.auto import tqdm\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"691146737a01402aa0d63f98d438ad7c","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=2781.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-334d1aac5ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"RC1etum4Tufx"},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]}]}