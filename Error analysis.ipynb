{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed584df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import requests, re, string, datetime, copy\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T, torch.nn.functional as F, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import logging\n",
    "datasets.logging.get_verbosity = lambda: logging.NOTSET\n",
    "\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback, TrainerCallback\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, get_linear_schedule_with_warmup\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from threading import Lock\n",
    "\n",
    "PATH = \"./data/\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a11db5",
   "metadata": {},
   "source": [
    "##### text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tokenization classes for Italian AlBERTo models.\"\"\"\n",
    "import collections\n",
    "import os\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'user', 'percent', 'money', 'phone', 'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\"},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "\n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "class AlBERTo_Preprocessing(object):\n",
    "    def __init__(self, do_lower_case=True, **kwargs):\n",
    "        self.do_lower_case = do_lower_case\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if self.do_lower_case:\n",
    "            text = text.lower()\n",
    "        text = str(\" \".join(text_processor.pre_process_doc(text)))\n",
    "        text = re.sub(r'[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
    "        text = re.sub(r'^\\s', '', text)\n",
    "        text = re.sub(r'\\s$', '', text)\n",
    "        return text\n",
    "\n",
    "a = AlBERTo_Preprocessing(do_lower_case=True)\n",
    "s = \"#IlGOverno presenta, le linee guida... sulla scuola! #labuonascuola - http://t.co/SYS1T9QmQN\"\n",
    "b = a.preprocess(s)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425069db",
   "metadata": {},
   "source": [
    "##### pretrained tokenizer and pretrained AlBERTo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe70768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    sa = tok(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    return sa\n",
    "\n",
    "a = AlBERTo_Preprocessing(do_lower_case=True)\n",
    "s: str = \"#IlGOverno presenta le linee guida sulla scuola #labuonascuola - http://t.co/SYS1T9QmQN\"\n",
    "b = a.preprocess(s)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "pretrained_model = AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "tok.model_max_length = 128 #model.config.max_position_embeddings\n",
    "tokens = tok.tokenize(b)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a167f55",
   "metadata": {},
   "source": [
    "##### models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beac59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetMC(nn.Module):\n",
    "    \"\"\"\n",
    "        Attach a FC layer on top of the BERT head in order to produce a classification output.\n",
    "\n",
    "        The pooled_output output of BERT is basically a projection of the [CLS] embeddings via another FC layer (768 -> 768 hidden units).\n",
    "        We stack another FC layer with Dropout on top of that, as reported in https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_classifier.py#L574\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels):\n",
    "        super(MyNetMC, self).__init__()\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.model = copy.deepcopy(pretrained_model)#AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.linear1 = nn.Linear(768, 3)\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, labels, input_ids, attention_mask, **args):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **args)\n",
    "        x = self.dropout1(outputs[1])\n",
    "        logits = self.linear1(x)\n",
    "        loss = self.loss_fct(logits, labels)\n",
    "        return logits , loss\n",
    "    \n",
    "class MyNetMCTuned(nn.Module):\n",
    "    \"\"\"\n",
    "        Attach a FC layer on top of the BERT head in order to produce a classification output.\n",
    "\n",
    "        The pooled_output output of BERT is basically a projection of the [CLS] embeddings via another FC layer (768 -> 768 hidden units).\n",
    "        We stack another FC layer without Dropout on top of that, as reported in https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_classifier.py#L574\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels):\n",
    "        super(MyNetMCTuned, self).__init__()\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.model = copy.deepcopy(pretrained_model)\n",
    "        self.linear = nn.Linear(768, 3)\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, labels, input_ids, attention_mask, **args):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **args)\n",
    "        logits = self.linear(outputs[1])\n",
    "        loss = self.loss_fct(logits, labels)\n",
    "        return logits , loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17cc4c1",
   "metadata": {},
   "source": [
    "#### EvSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evSent(string):\n",
    "    url = \"http://ai-rest.blupixelit.eu/rest.php?action=SentiLex&username=asr&password=asrpwd&lang=it&text=%s\"%string\n",
    "    try:\n",
    "        result = requests.get(url).text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise print(\"ERROR EVSENT\")\n",
    "    result = (re.findall(\"<sentiment>(.*?)</sentiment>\", result)[0])\n",
    "    if result == \"positive\":\n",
    "        return 2\n",
    "    elif result == \"negative\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def worker(label, text, pbar, preds, true, lock):\n",
    "    res = evSent(text)\n",
    "    lock.acquire()\n",
    "    global index\n",
    "    preds[index] = res\n",
    "    true[index] = label\n",
    "    index += 1\n",
    "    pbar.update(1)\n",
    "    lock.release()\n",
    "\n",
    "def evsent_remove_neutrals(true, preds):\n",
    "    preds2 , true2 = [] , []\n",
    "    for i in range(len(preds)):\n",
    "        if true[i] == 1:\n",
    "            continue\n",
    "        elif true[i] == 2:\n",
    "            true2.append(1)\n",
    "        else:\n",
    "            true2.append(0)\n",
    "        if preds[i] == 1:\n",
    "            preds2.append(0 if np.random.rand(1) > 0.50 else 1)\n",
    "        elif preds[i] == 2:\n",
    "            preds2.append(1)\n",
    "        else:\n",
    "            preds2.append(0) \n",
    "    print(classification_report(true2, preds2, target_names=[\"negative\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee0126",
   "metadata": {},
   "source": [
    "#####  utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def validation_epoch(model, epoch, val_loader, kind, logging):\n",
    "    model.eval()\n",
    "    targets , outputs , logitss = [] , [] , []\n",
    "    cumulative_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i , data in enumerate(val_loader, 0):\n",
    "            batch = {k: v.to(device) for k, v in data.items()}\n",
    "            logits , loss = model(**batch)\n",
    "            if kind != \"test\": cumulative_loss += loss.detach()\n",
    "            if (i+1) % 25 == 0 and logging:\n",
    "                print(f'Epoch: {epoch}, Loss:  {cumulative_loss.item()/i}')\n",
    "            \n",
    "            targets.extend(batch[\"labels\"].cpu().detach().numpy())\n",
    "            outputs.extend(logits.argmax(-1).cpu().detach().numpy().tolist())\n",
    "            logitss.extend(logits.cpu().detach().numpy().tolist())\n",
    "    if logging: wandb.log({kind: {'loss': cumulative_loss.item() / len(outputs)}})\n",
    "    return outputs, targets, logitss\n",
    "\n",
    "\n",
    "def fine_tune_feel_it(model, training, logging, new_model_num_labels):\n",
    "    if model.num_labels == 2:\n",
    "        model.num_labels = new_model_num_labels\n",
    "        model.classifier.out_proj = nn.Linear(768, new_model_num_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "                    \"test_trainer\", \n",
    "                    num_train_epochs=5,\n",
    "                    per_device_train_batch_size=32,\n",
    "                    save_total_limit=2,\n",
    "                    learning_rate=2e-5,\n",
    "                    warmup_ratio=0.1,\n",
    "                    weight_decay=0.01,\n",
    "                    adam_beta1=0.9,\n",
    "                    adam_beta2=0.999,\n",
    "                    adam_epsilon=1e-6,\n",
    "                    logging_strategy=\"epoch\",\n",
    "                    overwrite_output_dir=True,\n",
    "                    save_strategy=\"no\",\n",
    "                    report_to=\"none\",\n",
    "                )\n",
    "    trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args, \n",
    "                train_dataset=training, \n",
    "                compute_metrics=compute_metrics,\n",
    "            )\n",
    "    trainer.train()\n",
    "    return trainer.model\n",
    "\n",
    "\n",
    "def eval_feelit_model(dataset, keep_neutrals, dataset_name, plot_confidence=False, fine_tune=False, dataset_train=False, logging=False, model_i=None, model_num_labels=3):\n",
    "    def MC2binary(e):\n",
    "        #to convert the class label of positives from 2 into 1, when keep_neutrals=False and model_num_labels=2\n",
    "        if not keep_neutrals:\n",
    "            e[\"labels\"] = e[\"labels\"] if e[\"labels\"] == 0 else 1\n",
    "        return e\n",
    "    \n",
    "    tmp = \"no neutral\" if not keep_neutrals else \"\"\n",
    "    tmp2 = \"pretrained\" if not fine_tune and model_i is None else \"MC fine tuned\"\n",
    "    print(\"Feel-it \", tmp, \" \", tmp2, \" --- \", dataset_name)\n",
    "    \n",
    "    tok_feelit = AutoTokenizer.from_pretrained(\"MilaNLProc/feel-it-italian-sentiment\")\n",
    "    tok_feelit.model_max_length = 128\n",
    "\n",
    "    if model_i is None:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"MilaNLProc/feel-it-italian-sentiment\").to(device)\n",
    "    else:\n",
    "        model = model_i\n",
    "\n",
    "    if keep_neutrals:\n",
    "        testing = Dataset.from_pandas(dataset)\\\n",
    "                        .filter(lambda example: example['labels'] != 3)\\\n",
    "                        .map(lambda examples: tok_feelit(examples[\"text\"], padding=\"max_length\", truncation=True), batched=True)\\\n",
    "                        .with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    else:\n",
    "        testing = Dataset.from_pandas(dataset)\\\n",
    "                .map(lambda examples: tok_feelit(examples[\"text\"], padding=\"max_length\", truncation=True), batched=True)\\\n",
    "                .filter(lambda example: example['labels'] != 1)\\\n",
    "                .filter(lambda example: example['labels'] != 3)\\\n",
    "                .with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    test_loader = DataLoader(testing, batch_size=32)\n",
    "    \n",
    "    if fine_tune: #fine-tune while keeping neutral samples\n",
    "        training = Dataset.from_pandas(dataset_train)\\\n",
    "                            .filter(lambda example: example['labels'] != 3)\\\n",
    "                            .map(MC2binary, batched=False)\\\n",
    "                            .map(lambda examples: tok_feelit(examples[\"text\"], padding=\"max_length\", truncation=True), batched=True)\\\n",
    "                            .shuffle()\\\n",
    "                            .with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "        model = fine_tune_feel_it(model, training, logging, model_num_labels)\n",
    "\n",
    "    model.eval()\n",
    "    trues , preds , logitss = [] , [] , []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(test_loader, 0):\n",
    "            batch = {k: data[k].to(device) for k in ['attention_mask', 'input_ids']}\n",
    "            logits = model(**batch)[0]\n",
    "            trues.extend(data[\"labels\"].detach().numpy())\n",
    "            preds.extend(logits.argmax(-1).cpu().detach().numpy().tolist())\n",
    "            logitss.extend(logits.cpu().detach().numpy().tolist())\n",
    "    \n",
    "    trues = np.array(trues)\n",
    "    preds = np.array(preds)\n",
    "    proba = torch.nn.functional.softmax(torch.tensor(logitss), dim=1)\n",
    "    assert len(preds) == len(proba) #and np.all(np.array((proba[:,1] + proba[:,0]) >= 0.99))\n",
    "    num_assigned_to_neutral , num_correclty_assigned_to_neutral = 0 , 0\n",
    "    for i in range(len(proba)):\n",
    "        if keep_neutrals and not fine_tune and model_i is None:\n",
    "            if preds[i] == 1: \n",
    "                preds[i] = 2\n",
    "            if max(proba[i,0] , proba[i, 1]) <= 0.65: #set to neutral  if the prediction's confidence is too low\n",
    "                preds[i] = 1\n",
    "                num_assigned_to_neutral += 1\n",
    "                if trues[i] == 1:\n",
    "                    num_correclty_assigned_to_neutral += 1\n",
    "        elif not keep_neutrals:\n",
    "            if (fine_tune or model_i) and model_num_labels == 3:\n",
    "                raise ValueError(\"Removing neutrals for fine-tuned model not implemented\")\n",
    "            elif (fine_tune or model_i) and model_num_labels == 2 and trues[i] == 2: \n",
    "                trues[i] = 1\n",
    "            elif not fine_tune and model_i is None and trues[i] == 2:\n",
    "                trues[i] = 1\n",
    "\n",
    "    targets = [\"negative\", \"neutral\", \"positive\"] if keep_neutrals else [\"negative\", \"positive\"]\n",
    "    print(classification_report(trues, preds, target_names=targets))\n",
    "    cm = confusion_matrix(trues, preds, normalize='all')\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=targets).plot()\n",
    "    plt.title(f\"{dataset_name} {tmp} - Feel-it {tmp2}\")\n",
    "    if keep_neutrals:\n",
    "        print(f\"\\n\\n{num_assigned_to_neutral} samples were assigned to the class neutral, of which {num_correclty_assigned_to_neutral} were correct, out of {sum((trues) == 1)} neutrals\\n\\n\")\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x=proba[:, 0], \n",
    "                               xbins=dict(\n",
    "                                start=0.,\n",
    "                                end=1.0,\n",
    "                                size=0.1\n",
    "                                ), \n",
    "                               histnorm='probability'))\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=670,\n",
    "        height=300,\n",
    "        xaxis_title=\"confidence\",\n",
    "        yaxis_title=\"normalized count\",\n",
    "    )\n",
    "    fig.update_xaxes(range=[0., 1.])\n",
    "    fig.show()\n",
    "\n",
    "    if plot_confidence:\n",
    "        plot_confidence_distribution(trues, preds, proba, name=f\"Feel-it model {tmp2}\", dataset=dataset_name + f\" {tmp}\", plot_neutral=keep_neutrals)\n",
    "    if fine_tune:\n",
    "        return model\n",
    "        \n",
    "        \n",
    "def train_epoch(model, train_loader, optimizer, scheduler, epoch, logging):\n",
    "    model.train()\n",
    "    targets , outputs = [] , []\n",
    "    cumulative_loss = 0.\n",
    "    for i , data in enumerate(train_loader, 0): \n",
    "        targets.extend(data[\"labels\"].numpy())\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in data.items()}\n",
    "        logits , loss = model(**batch)\n",
    "\n",
    "        cumulative_loss += loss.detach()\n",
    "        if (i+1) % 25 == 0 and logging:\n",
    "            print(f'Epoch: {epoch}, Loss:  {cumulative_loss.item()/i}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        outputs.extend(logits.argmax(-1).cpu().detach().numpy().tolist())\n",
    "    return outputs, targets\n",
    "\n",
    "def fine_tune_alberto(train_loader, model, logging, model_type, dataset_val=None):\n",
    "    model.train()\n",
    "    if model_type == \"opt\":\n",
    "        hyper = {'learning_rate': 3.000003529363845e-06, 'warmup_steps': 0.6, 'weight_decay': 0.000260393798851559}\n",
    "    elif model_type == \"MC\":\n",
    "        hyper = {'learning_rate': 2e-5, 'warmup_steps': 0.1, 'weight_decay': 0.1}\n",
    "        \n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=hyper[\"learning_rate\"],\n",
    "                              weight_decay=hyper[\"weight_decay\"], \n",
    "                              eps=1e-6)\n",
    "    num_epochs = 5\n",
    "    num_train_steps = int(len(train_loader) * num_epochs) + 1\n",
    "    num_warmup_steps =  int(num_train_steps * hyper[\"warmup_steps\"])\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        outputs, targets = train_epoch(model, train_loader, optimizer, scheduler, epoch, logging)\n",
    "        if logging: \n",
    "            f1_train = f1_score(targets, outputs, average='macro')\n",
    "            print(f\"Train F1 = {f1_train}\")\n",
    "    return model\n",
    "        \n",
    "\n",
    "def eval_alberto(model, model_name, model_params, dataset, dataset_name, keep_neutrals, plot_confidence, return_preds=False, fine_tuning=False, dataset_train=None, logging=False, model_type=None, model_init=None):\n",
    "    tmp = \" no neutrals\" if not keep_neutrals else \" \"\n",
    "    tmp2 = \" fine tuned\" if fine_tuning else \" \"\n",
    "    print(model_name, tmp, tmp2, \" --- \", dataset_name)\n",
    "    \n",
    "    if keep_neutrals:\n",
    "        testing = Dataset.from_pandas(dataset)\\\n",
    "                        .filter(lambda example: example['labels'] != 3)\\\n",
    "                        .map(tokenize_function, batched=True)\\\n",
    "                        .with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"])\n",
    "    else:\n",
    "        testing = Dataset.from_pandas(dataset)\\\n",
    "                        .filter(lambda example: example['labels'] != 1)\\\n",
    "                        .filter(lambda example: example['labels'] != 3)\\\n",
    "                        .map(tokenize_function, batched=True)\\\n",
    "                        .with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"])\n",
    "    test_loader = DataLoader(testing, batch_size=32)\n",
    "\n",
    "    if model_init is None:\n",
    "        model = model(num_labels=3).to(device)\n",
    "        model.load_state_dict(torch.load(PATH + model_params))\n",
    "    else:\n",
    "        model = model_init\n",
    "    \n",
    "    if fine_tuning:\n",
    "        if keep_neutrals:\n",
    "            training = Dataset.from_pandas(dataset_train)\\\n",
    "                            .filter(lambda example: example['labels'] != 3)\\\n",
    "                            .map(tokenize_function, batched=True)\\\n",
    "                            .with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"])\n",
    "        else:\n",
    "            training = Dataset.from_pandas(dataset_train)\\\n",
    "                            .filter(lambda example: example['labels'] != 1)\\\n",
    "                            .filter(lambda example: example['labels'] != 3)\\\n",
    "                            .map(tokenize_function, batched=True)\\\n",
    "                            .with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"])\n",
    "        train_loader = DataLoader(training, batch_size=64, shuffle=True)\n",
    "        model = fine_tune_alberto(train_loader, model, logging, model_type)\n",
    "\n",
    "    model.eval()\n",
    "    preds , trues , logitss = validation_epoch(model, None, test_loader, \"test\", logging=False)\n",
    "    proba = torch.nn.functional.softmax(torch.tensor(logitss), dim=1)\n",
    "    for i in range(len(preds)):\n",
    "        if not keep_neutrals:\n",
    "            if trues[i] == 2: \n",
    "                 trues[i] = 1\n",
    "            if preds[i] == 1: #reassign prediction to second most-probable prediction\n",
    "                idx = np.argmax([proba[i][0], proba[i][2]])\n",
    "                assert proba[i][0 if idx == 0 else 2] > proba[i][0 if idx == 1 else 2] and proba[i][0 if idx == 0 else 2] != 0 #to check that proba[i][0] != proba[i][2] != 0\n",
    "                preds[i] = idx\n",
    "            elif preds[i] == 2:\n",
    "                preds[i] = 1\n",
    "                \n",
    "    targets = [\"negative\", \"neutral\", \"positive\"] if keep_neutrals else [\"negative\", \"positive\"]\n",
    "    print(classification_report(trues, preds, target_names=targets))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(trues, preds, normalize='all'), display_labels=targets).plot()\n",
    "    plt.title(f\"{dataset_name} - {model_name} {tmp}\")\n",
    "    \n",
    "    trues = np.array(trues)\n",
    "    preds = np.array(preds)\n",
    "        \n",
    "    if plot_confidence:\n",
    "        plot_confidence_distribution(trues, preds, proba, name=model_name, dataset=dataset_name, plot_neutral=keep_neutrals)\n",
    "    if return_preds:\n",
    "        return preds, trues, proba\n",
    "    if fine_tuning:\n",
    "        return model\n",
    "    \n",
    "    \n",
    "def eval_mfc_baseline(dataset_train, dataset_test, keep_neutrals):\n",
    "    if not keep_neutrals:\n",
    "        dataset_train = dataset_train[dataset_train.labels != 1]\n",
    "        dataset_test = dataset_test[dataset_test.labels != 1]\n",
    "    dataset_train = dataset_train[dataset_train.labels != 3]\n",
    "    dataset_test = dataset_test[dataset_test.labels != 3]\n",
    "    \n",
    "    dummy_clf = DummyClassifier(strategy=\"stratified\").fit(dataset_train.text, dataset_train.labels)\n",
    "    preds = dummy_clf.predict(dataset_test.text)\n",
    "    \n",
    "    targets = [\"negative\", \"neutral\", \"positive\"] if keep_neutrals else [\"negative\", \"positive\"]\n",
    "    print(classification_report(dataset_test.labels, preds, target_names=targets))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(dataset_test.labels, preds, normalize='all'), display_labels=targets).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e4e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidence_distribution(trues, preds, proba, name, dataset, plot_neutral):\n",
    "    false_negatives = np.logical_and(trues != 0, preds == 0)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x=proba[false_negatives, np.argmax(proba[false_negatives,:], axis=1)], \n",
    "                               xbins=dict(\n",
    "                                start=0.,\n",
    "                                end=1.0,\n",
    "                                size=0.1\n",
    "                                ), \n",
    "                               histnorm='probability'))\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=670,\n",
    "        height=300,\n",
    "        title=f\"{name}'s confidence distribution for {dataset} - false negatives\",\n",
    "        xaxis_title=\"confidence\",\n",
    "        yaxis_title=\"normalized count\",\n",
    "    )\n",
    "    fig.update_xaxes(range=[0., 1.])\n",
    "    fig.show()\n",
    "\n",
    "    if plot_neutral:\n",
    "        false_neutrals = np.logical_and(trues != 1, preds == 1)\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Histogram(x=proba[false_neutrals, np.argmax(proba[false_neutrals,:], axis=1)], \n",
    "                                   xbins=dict(\n",
    "                                    start=0.,\n",
    "                                    end=1.0,\n",
    "                                    size=0.1\n",
    "                                    ), \n",
    "                                   histnorm='probability'))\n",
    "        fig.update_layout(\n",
    "            autosize=False,\n",
    "            width=670,\n",
    "            height=300,\n",
    "            title=f\"{name}'s confidence distribution for {dataset} - false neutrals\",\n",
    "            xaxis_title=\"confidence\",\n",
    "            yaxis_title=\"normalized count\",\n",
    "        )\n",
    "        fig.update_xaxes(range=[0., 1.])\n",
    "        fig.show()\n",
    "\n",
    "    false_positives = np.logical_and(trues != 2, preds == 2) if plot_neutral else np.logical_and(trues != 1, preds == 1)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(x=proba[false_positives, np.argmax(proba[false_positives,:], axis=1)], \n",
    "                               xbins=dict(\n",
    "                                start=0.,\n",
    "                                end=1.0,\n",
    "                                size=0.1\n",
    "                                ), \n",
    "                               histnorm='probability'))\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=670,\n",
    "        height=300,\n",
    "        title=f\"{name}'s confidence distribution for {dataset} - false positives\",\n",
    "        xaxis_title=\"confidence\",\n",
    "        yaxis_title=\"normalized count\",\n",
    "    )\n",
    "    fig.update_xaxes(range=[0., 1.])\n",
    "    fig.show()\n",
    "    \n",
    "    \n",
    "def get_lexicon(dataset):\n",
    "    \"\"\"\n",
    "    Takes as input an array containing already pre-processed text \n",
    "    \"\"\"\n",
    "    return set([token for sentence in dataset for token in sentence.split(\" \")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be6204f",
   "metadata": {},
   "source": [
    "# SENTIPOLC16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentipolc = pd.read_csv(PATH + \"Sentipolc16/training_set_sentipolc16.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57bfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(PATH + \"Sentipolc16/test_set_sentipolc16_gold2000.csv\", 'r') \n",
    "Lines = file1.readlines()\n",
    " \n",
    "test_sentipolc = []\n",
    "for line in Lines:\n",
    "    arr = line.split(\"\\\",\")\n",
    "    if len(arr) != 9:\n",
    "        arr[8] = arr[8] + arr[9]  #to account for tweets containing the delimiter charachter that would create more splits than needed\n",
    "        del arr[9:]\n",
    "    for i in range(8):\n",
    "        arr[i] = int(arr[i].strip(\"\\\"\"))\n",
    "    test_sentipolc.append(arr)\n",
    "\n",
    "test_sentipolc = pd.DataFrame(test_sentipolc, columns=train_sentipolc.columns)\n",
    "test_sentipolc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb97c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate2united_labels(row):\n",
    "    \"\"\"\n",
    "        Return a single scalar integer label associated to the polarity of the tweet.\n",
    "\n",
    "        Negative -> 0\n",
    "        Neutral  -> 1\n",
    "        Positive -> 2\n",
    "        Mixed    -> 3\n",
    "    \"\"\"\n",
    "    if row[\"opos\"] == 0 and row[\"oneg\"] == 0:\n",
    "        return 1\n",
    "    elif row[\"oneg\"] == 0 and row[\"opos\"] == 1:\n",
    "        return 2\n",
    "    elif row[\"oneg\"] == 1 and row[\"opos\"] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "dataset_sentipolc_train = pd.DataFrame({\"text\": train_sentipolc.text.apply(a.preprocess), \"idx\": train_sentipolc.index, \"labels\": train_sentipolc[[\"opos\", \"oneg\"]].apply(separate2united_labels, axis=1)})\n",
    "dataset_sentipolc_test  = pd.DataFrame({\"text\": test_sentipolc.text.apply(a.preprocess), \"idx\": test_sentipolc.index, \"labels\": test_sentipolc[[\"opos\", \"oneg\"]].apply(separate2united_labels, axis=1)})\n",
    "print(\"train: \", len(dataset_sentipolc_train[dataset_sentipolc_train.labels != 3]), np.unique(dataset_sentipolc_train[dataset_sentipolc_train.labels != 3].labels, return_counts=True)[1]/len(dataset_sentipolc_train[dataset_sentipolc_train.labels != 3]))\n",
    "print(\"Test: \", len(dataset_sentipolc_test[dataset_sentipolc_test.labels != 3]), np.unique(dataset_sentipolc_test[dataset_sentipolc_test.labels != 3].labels, return_counts=True)[1]/len(dataset_sentipolc_test[dataset_sentipolc_test.labels != 3]))\n",
    "lexicon_sentipolc = get_lexicon(dataset_sentipolc_test.text)\n",
    "\n",
    "tmp = pd.concat([dataset_sentipolc_train[dataset_sentipolc_train.labels != 3], dataset_sentipolc_test[dataset_sentipolc_test.labels != 3]])\n",
    "print(\"All: \", len(tmp), np.unique(tmp.labels, return_counts=True)[1]/len(tmp))\n",
    "print(\"Size lexicon all: \", len(get_lexicon(tmp.text)))\n",
    "print(\"Avg num tokens: \", round(np.mean(tmp[\"text\"].apply(lambda x: len(x.split(\" \")))), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98484ba",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11e3826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%script echo skipping\n",
    "examples = dataset_sentipolc_test[dataset_sentipolc_test.labels != 3] # exclude mixed tweets\n",
    "\n",
    "preds , true , pbar = np.full(len(examples), -1) , np.full(len(examples), -1) , tqdm(total=len(examples))\n",
    "index , lock , p = 0 , Lock() , ThreadPool(processes=6)\n",
    "for text , label in zip(examples.text, examples.labels):\n",
    "    p.apply_async(worker, (label, text, pbar, preds, true, lock))\n",
    "p.close(); p.join()\n",
    "pbar.close()\n",
    "assert not np.any(preds == -1) and not np.any(true == -1)\n",
    "print(classification_report(true, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "##\n",
    "# No Neutrals\n",
    "# reassing labels so that no neutrals are present\n",
    "##\n",
    "print(\"\\n\\n No neutrals\\n\")\n",
    "evsent_remove_neutrals(true, preds)\n",
    "\n",
    "\n",
    "##\n",
    "# Stochastic Most Frequent Classifier\n",
    "##\n",
    "print(\"\\n\\n SMFC\\n\")\n",
    "eval_mfc_baseline(dataset_sentipolc_train, dataset_sentipolc_test, keep_neutrals=True)\n",
    "\n",
    "print(\"\\n\\n SMFC no neutrals\\n\")\n",
    "eval_mfc_baseline(dataset_sentipolc_train, dataset_sentipolc_test, keep_neutrals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f902fb7",
   "metadata": {},
   "source": [
    "### AlBERTo pretrained - SENTIPOLC16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5c7a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds , trues , proba = eval_alberto(MyNetMC, \"AlBERTo MC\", \"models/alberto_multiclass.pt\", dataset_sentipolc_test, \"Sentipolc16\", return_preds=True, keep_neutrals=True, plot_confidence=True)\n",
    "eval_alberto(MyNetMC, \"AlBERTo MC\", \"models/alberto_multiclass.pt\", dataset_sentipolc_test, \"Sentipolc16\", keep_neutrals=False, plot_confidence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b91576",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(trues == np.array(test_sentipolc[[\"opos\", \"oneg\"]].apply(separate2united_labels, axis=1)[dataset_sentipolc_test.labels != 3].tolist()))\n",
    "irony = np.array(test_sentipolc.iro.tolist())[dataset_sentipolc_test.labels != 3][preds != trues]\n",
    "\n",
    "print(\"Irony distribution in the prediction errors: \", np.unique(irony, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca764f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(preds)):\n",
    "    if preds[i] == 0 and trues[i] == 0:\n",
    "        print(\"\\n\\n\", dataset_sentipolc_test[dataset_sentipolc_test.labels != 3].text.tolist()[i], i)\n",
    "        print(\"Pred=\", preds[i])\n",
    "        print(\"True=\", trues[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c7fb0",
   "metadata": {},
   "source": [
    "### AlBERTo opt - SENTIPOLC16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd63486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", dataset_sentipolc_test, \"Sentipolc16\", keep_neutrals=True, plot_confidence=True)\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", dataset_sentipolc_test, \"Sentipolc16\", keep_neutrals=False, plot_confidence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396db52",
   "metadata": {},
   "source": [
    "### Feel-it model - SENTIPOLC16\n",
    "Note that it was trained just for positive/negative class, so the neutral class is taken for examples with low confidence in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161267ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(dataset_sentipolc_test, keep_neutrals=True, dataset_name=\"Sentipolc16\", plot_confidence=True)\n",
    "eval_feelit_model(dataset_sentipolc_test, keep_neutrals=False, dataset_name=\"Sentipolc16\", plot_confidence=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ddd87",
   "metadata": {},
   "source": [
    "### Feel-it MC fine tuned - SENTIPOLC16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73722522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(dataset_sentipolc_test, keep_neutrals=True, fine_tune=True, dataset_train=dataset_sentipolc_train, dataset_name=\"Sentipolc16\", plot_confidence=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e10b64",
   "metadata": {},
   "source": [
    "# FEEL-IT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feel_it = pd.read_csv(PATH + \"Feel-it/feelit.tsv\", sep='\\t', header=0, names=[\"text\", \"label\"])\n",
    "print(\"\\nAverage tweet lenght: \", np.mean(train_feel_it[\"text\"].apply(len)))\n",
    "train_feel_it.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723dd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion2sentiment(row):\n",
    "    \"\"\"\n",
    "        Return a single scalar integer label associated to the emotion of the tweet.\n",
    "\n",
    "        joy -> 2\n",
    "        anger sadness fear  -> 0\n",
    "    \"\"\"\n",
    "    if row[\"label\"] == \"joy\":\n",
    "        return 2\n",
    "    elif row[\"label\"] == \"sadness\" or row[\"label\"] == \"fear\" or row[\"label\"] == \"anger\":\n",
    "        return 0\n",
    "    else:\n",
    "        raise ValueError('Value not recognized')\n",
    "\n",
    "#train set\n",
    "dataset_feel_it = pd.DataFrame({\"text\": train_feel_it.text.apply(a.preprocess), \"idx\": train_feel_it.index, \"labels\": train_feel_it[[\"label\"]].apply(emotion2sentiment, axis=1)})\n",
    "print(\"Test: \", len(dataset_feel_it), np.unique(dataset_feel_it.labels, return_counts=True)[1]/len(dataset_feel_it))\n",
    "lexicon_feel_it = get_lexicon(dataset_feel_it.text)\n",
    "print(\"Sentipolc lexicon  ∩ Feel-it lexicon: \", round(len(lexicon_feel_it.intersection(lexicon_sentipolc))/len(lexicon_feel_it.union(lexicon_sentipolc)),2))\n",
    "\n",
    "tmp = dataset_feel_it\n",
    "print(\"All: \", len(tmp), np.unique(tmp.labels, return_counts=True)[1]/len(tmp))\n",
    "print(\"Size lexicon all: \", len(get_lexicon(tmp.text)))\n",
    "print(\"Avg num tokens: \", round(np.mean(tmp[\"text\"].apply(lambda x: len(x.split(\" \")))), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ad919",
   "metadata": {},
   "source": [
    "### Evsent baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32966f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "examples = dataset_feel_it\n",
    "\n",
    "preds , true , pbar = np.full(len(examples), -1) , np.full(len(examples), -1) , tqdm(total=len(examples))\n",
    "index , lock , p = 0 , Lock() , ThreadPool(processes=10)\n",
    "for text , label in zip(examples.text, examples.labels):\n",
    "    p.apply_async(worker, (label, text, pbar, preds, true, lock))\n",
    "p.close(); p.join()\n",
    "pbar.close()\n",
    "assert not np.any(preds == -1) and not np.any(true == -1)\n",
    "print(classification_report(true, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "##\n",
    "# No Neutrals\n",
    "# reassing labels so that no neutrals are present\n",
    "##\n",
    "print(\"\\n\\n No neutrals\\n\")\n",
    "evsent_remove_neutrals(true, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b34cdd",
   "metadata": {},
   "source": [
    "### AlBERTo pretrained - FEEL-IT\n",
    "Recall that FEEL_IT does not have neutral tweets, but AlBERTo was trained with such class. So reassign neutral predictions to the second most probable prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5dd7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds , trues , proba = eval_alberto(MyNetMC, \"AlBERTo MC\", \"models/alberto_multiclass.pt\", dataset_feel_it, \"Feel-it\", return_preds=True, keep_neutrals=False, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba7f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(preds)):\n",
    "    if preds[i] == 1 and trues[i] == 1:\n",
    "        print(\"\\n\\n\", dataset_feel_it[dataset_feel_it.labels != 3].text.tolist()[i], i)\n",
    "        print(\"Pred=\", preds[i])\n",
    "        print(\"True=\", trues[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4d714",
   "metadata": {},
   "source": [
    "### AlBERTo opt - Feel-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9993edb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", dataset_feel_it, \"Feel-it\", keep_neutrals=False, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4362b",
   "metadata": {},
   "source": [
    "### FEEL_IT model - FEEL_IT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf1990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(dataset_feel_it, keep_neutrals=False, dataset_name=\"Feel-IT\", plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e9615",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0fd763",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# To split the dataset, one split, read the directly the 2 files\n",
    "##\n",
    "\n",
    "# amazon = pd.read_json(PATH + \"Amazon-reviews/Amazon_reviews_computer_cloths_food_shoaps.json\")\n",
    "# X_train, X_test = train_test_split(amazon, test_size=0.3, random_state=42, stratify=amazon[\"rating\"])\n",
    "# X_train.to_json(PATH + \"Amazon-reviews/amazon_train.json\")\n",
    "# X_test.to_json(PATH + \"Amazon-reviews/amazon_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a16c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_train = pd.read_json(PATH + \"Amazon-reviews/amazon_train.json\")\n",
    "amazon_test = pd.read_json(PATH + \"Amazon-reviews/amazon_test.json\")\n",
    "\n",
    "amazon_train[\"text\"] = amazon_train[\"title\"] + \": \" +  amazon_train[\"text\"]\n",
    "amazon_test[\"text\"] = amazon_test[\"title\"] + \": \" +  amazon_test[\"text\"]\n",
    "\n",
    "print(\"Train: \", len(amazon_train) , np.unique(amazon_train.rating, return_counts=True)[1]/len(amazon_train))\n",
    "print(\"Test:\", len(amazon_test) , np.unique(amazon_test.rating, return_counts=True)[1]/len(amazon_test))\n",
    "\n",
    "print(\"\\nAverage review lenght Train: \", np.mean(amazon_train[\"text\"].apply(len)))\n",
    "print(\"\\nAverage review lenght Test: \", np.mean(amazon_test[\"text\"].apply(len)))\n",
    "\n",
    "amazon_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c1b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_test[amazon_test.text.str.contains(\"Preso durante\")].iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af844060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating2sentiment(row):\n",
    "    \"\"\"\n",
    "        Returns the polarity depending on the rating of the review\n",
    "    \"\"\"\n",
    "    if row[\"rating\"] == 3:\n",
    "        return 1\n",
    "    elif row[\"rating\"] > 3:\n",
    "        return 2\n",
    "    elif row[\"rating\"] < 3:\n",
    "        return 0\n",
    "    \n",
    "#tok = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "#tok.model_max_length = 400\n",
    "\n",
    "dataset_amazon_train = pd.DataFrame({\"text\": amazon_train.text.apply(a.preprocess), \"idx\": amazon_train.index, \"labels\": amazon_train[[\"rating\"]].apply(rating2sentiment, axis=1)}) \n",
    "dataset_amazon_test  = pd.DataFrame({\"text\": amazon_test.text.apply(a.preprocess), \"idx\": amazon_test.index, \"labels\": amazon_test[[\"rating\"]].apply(rating2sentiment, axis=1)}) \n",
    "print(\"Train: \", len(dataset_amazon_train), np.unique(dataset_amazon_train.labels, return_counts=True)[1]/len(dataset_amazon_train))\n",
    "print(\"Test: \", len(dataset_amazon_test), np.unique(dataset_amazon_test.labels, return_counts=True)[1]/len(dataset_amazon_test))\n",
    "lexicon_amazon = get_lexicon(dataset_amazon_test.text)\n",
    "print(\"Amazon lexicon  ∩ Feel-it lexicon: \", round(len(lexicon_amazon.intersection(lexicon_feel_it))/len(lexicon_amazon.union(lexicon_feel_it)),2))\n",
    "print(\"Amazon lexicon  ∩ Sentipolc lexicon: \", round(len(lexicon_amazon.intersection(lexicon_sentipolc))/len(lexicon_amazon.union(lexicon_sentipolc)),2))\n",
    "\n",
    "tmp = pd.concat([dataset_amazon_train, dataset_amazon_test])\n",
    "print(\"All: \", len(tmp), np.unique(tmp.labels, return_counts=True)[1]/len(tmp))\n",
    "print(\"Size lexicon all: \", len(get_lexicon(tmp.text)))\n",
    "print(\"Avg num tokens: \", round(np.mean(tmp[\"text\"].apply(lambda x: len(x.split(\" \")))), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06beec46",
   "metadata": {},
   "source": [
    "### EvSent baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a1a5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "examples = dataset_amazon_test\n",
    "\n",
    "preds , true , pbar = np.full(len(examples), -1) , np.full(len(examples), -1) , tqdm(total=len(examples))\n",
    "index , lock , p= 0 , Lock() , ThreadPool(processes=10)\n",
    "for text , label in zip(examples.text, examples.labels):\n",
    "    p.apply_async(worker, (label, text[:1987], pbar, preds, true, lock)) #[:1987] to avoid too long URL\n",
    "p.close(); p.join()\n",
    "pbar.close()\n",
    "assert not np.any(preds == -1) and not np.any(true == -1)\n",
    "print(classification_report(true, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "##\n",
    "# No Neutrals\n",
    "# reassing labels so that no neutrals are present\n",
    "##\n",
    "print(\"\\n\\n No neutrals\\n\")\n",
    "evsent_remove_neutrals(true, preds)\n",
    "\n",
    "##\n",
    "# Stochastic\n",
    "##\n",
    "print(\"\\n\\n SMFC\\n\")\n",
    "eval_mfc_baseline(dataset_amazon_train, dataset_amazon_test, keep_neutrals=True)\n",
    "\n",
    "print(\"\\n\\n SMFC no neutrals\\n\")\n",
    "eval_mfc_baseline(dataset_amazon_train, dataset_amazon_test, keep_neutrals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e74134",
   "metadata": {},
   "source": [
    "### AlBERTo MC/opt - Amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb1eac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds1 , trues1 , proba = eval_alberto(MyNetMC,      \"AlBERTo MC\", \"models/alberto_multiclass.pt\", dataset_amazon_test, \"Amazon reviews\", return_preds=True, keep_neutrals=True, plot_confidence=False)\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", dataset_amazon_test, \"Amazon reviews\", keep_neutrals=True, plot_confidence=False)\n",
    "eval_alberto(MyNetMC,      \"AlBERTo MC\", \"models/alberto_multiclass.pt\", dataset_amazon_test, \"Amazon reviews\", keep_neutrals=False, plot_confidence=False)\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", dataset_amazon_test, \"Amazon reviews\", keep_neutrals=False, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b203f3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(preds)):\n",
    "    if preds1[i] == 0 and trues1[i] == 0:\n",
    "        print(\"\\n\\n\", dataset_amazon_test.text.tolist()[i], i)\n",
    "        print(\"Pred=\", preds1[i])\n",
    "        print(\"True=\", trues1[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8207cbd",
   "metadata": {},
   "source": [
    "### AlBERTo MC/opt fine tuned - Amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6dd999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds , trues , proba = eval_alberto(MyNetMC, \n",
    "                             \"AlBERTo MC\", \n",
    "                             \"models/alberto_multiclass.pt\", \n",
    "                             dataset_amazon_test, \n",
    "                             \"Amazon reviews\", \n",
    "                             return_preds=True,\n",
    "                             keep_neutrals=True, \n",
    "                             plot_confidence=False, \n",
    "                             fine_tuning=True, \n",
    "                             dataset_train=dataset_amazon_train,\n",
    "                             logging=False,\n",
    "                             model_type=\"MC\"\n",
    "                            )\n",
    "\n",
    "eval_alberto(MyNetMCTuned, \n",
    "             \"AlBERTo MC opt\", \n",
    "             \"models/alberto_multiclass_tuned.pt\", \n",
    "             dataset_amazon_test, \n",
    "             \"Amazon reviews\", \n",
    "             keep_neutrals=True, \n",
    "             plot_confidence=False, \n",
    "             fine_tuning=True, \n",
    "             dataset_train=dataset_amazon_train,\n",
    "             logging=False,\n",
    "             model_type=\"opt\"\n",
    "            )\n",
    "\n",
    "eval_alberto(MyNetMC, \n",
    "             \"AlBERTo MC\", \n",
    "             \"models/alberto_multiclass.pt\", \n",
    "             dataset_amazon_test, \n",
    "             \"Amazon reviews\", \n",
    "             keep_neutrals=False, \n",
    "             plot_confidence=False, \n",
    "             fine_tuning=True, \n",
    "             dataset_train=dataset_amazon_train,\n",
    "             logging=False,\n",
    "             model_type=\"MC\"\n",
    "            )\n",
    "\n",
    "eval_alberto(MyNetMCTuned, \n",
    "                 \"AlBERTo MC opt\", \n",
    "                 \"models/alberto_multiclass_tuned.pt\", \n",
    "                 dataset_amazon_test, \n",
    "                 \"Amazon reviews\", \n",
    "                 keep_neutrals=False, \n",
    "                 plot_confidence=False, \n",
    "                 fine_tuning=True, \n",
    "                 dataset_train=dataset_amazon_train,\n",
    "                 logging=False,\n",
    "                 model_type=\"opt\"\n",
    "                );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd556c2",
   "metadata": {},
   "source": [
    "###### inspect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada306f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 193\n",
    "dataset_amazon_test.text.tolist()[idx] , dataset_amazon_test.labels.tolist()[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55025725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(preds)):\n",
    "    if preds[i] == trues[i] and trues[i] == 1:\n",
    "        print(\"\\n\\n\", dataset_amazon_test.text.tolist()[i], i)\n",
    "        print(\"Pred=\", preds[i])\n",
    "        print(\"True=\", trues[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed397f",
   "metadata": {},
   "source": [
    "### AlBERTo opt - Amazon reviews with stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4aca81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class AlBERTo_Preprocessing_Stop_words(object):\n",
    "    def __init__(self, do_lower_case=True, **kwargs):\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.stop_words = set(stopwords.words('italian'))\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if self.do_lower_case:\n",
    "            text = text.lower()\n",
    "        text = text_processor.pre_process_doc(text)\n",
    "        filtered_text = [w for w in text if not w in self.stop_words]\n",
    "        text = str(\" \".join(filtered_text))\n",
    "        text = re.sub(r'[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
    "        text = re.sub(r'^\\s', '', text)\n",
    "        text = re.sub(r'\\s$', '', text)\n",
    "        return text\n",
    "\n",
    "b = AlBERTo_Preprocessing_Stop_words(do_lower_case=True)\n",
    "\n",
    "dataset_amazon = pd.DataFrame({\"text\": amazon_test.text.apply(b.preprocess), \"idx\": amazon_test.index, \"labels\": amazon_test[[\"rating\"]].apply(rating2sentiment, axis=1)})\n",
    "print(\"\\n\\n AlBERTo MC opt no stop words\\n\\n\")\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt no stop words\", \"models/alberto_multiclass_tuned.pt\", dataset_amazon, \"Amazon reviews\", keep_neutrals=True, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8e8f53",
   "metadata": {},
   "source": [
    "### Feel-it - Amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80516eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(dataset_amazon_test, keep_neutrals=False, dataset_name=\"Amazon reviews\", plot_confidence=False)\n",
    "eval_feelit_model(dataset_amazon_test, keep_neutrals=True, dataset_name=\"Amazon reviews\", plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d252971",
   "metadata": {},
   "source": [
    "### Feel-it MC fine tuned - Amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c4792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(dataset_amazon_test, keep_neutrals=True, fine_tune=True, dataset_train=dataset_amazon_train, dataset_name=\"Amazon reviews\", plot_confidence=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82336d",
   "metadata": {},
   "source": [
    "---\n",
    "# MultiEmotions-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate2united_labels(row):\n",
    "    \"\"\"\n",
    "        Return a single scalar integer label associated to the polarity of the tweet.\n",
    "\n",
    "        Negative -> 0\n",
    "        Neutral  -> 1\n",
    "        Positive -> 2\n",
    "        Mixed/UNRELATED    -> 3\n",
    "    \"\"\"\n",
    "    if row[\"POS\"] == 0 and row[\"NEG\"] == 0 and row[\"NEUT\"] == 1:\n",
    "        return 1\n",
    "    elif row[\"NEG\"] == 0 and row[\"POS\"] == 1:\n",
    "        return 2\n",
    "    elif row[\"NEG\"] == 1 and row[\"POS\"] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "##\n",
    "# To split the dataset, one split, read the directly the 2 files\n",
    "##\n",
    "# me_train = pd.read_csv(PATH + \"Multiemotions-it/Multiemotions-it.tsv\", sep='\\t')\n",
    "# me_train[\"labels\"] = me_train[[\"POS\", \"NEG\", \"NEUT\"]].apply(separate2united_labels, axis=1)\n",
    "\n",
    "# X_train, X_test = train_test_split(me_train, test_size=0.15, random_state=42, stratify=me_train[\"labels\"])\n",
    "# X_train.to_csv(PATH + \"Multiemotions-it/Multiemotions-it_train.tsv\", sep='\\t', index=False)\n",
    "# X_test.to_csv(PATH + \"Multiemotions-it/Multiemotions-it_test.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e36965",
   "metadata": {},
   "outputs": [],
   "source": [
    "me_train = pd.read_csv(PATH + \"Multiemotions-it/Multiemotions-it_train.tsv\", sep='\\t')\n",
    "me_test = pd.read_csv(PATH + \"Multiemotions-it/Multiemotions-it_test.tsv\", sep='\\t')\n",
    "\n",
    "me_train[\"text\"] = me_train[\"comment\"] \n",
    "me_test[\"text\"] = me_test[\"comment\"]\n",
    "\n",
    "me_train.drop(['type', 'title', 'URL', 'comment', 'UNRELATED', 'NEUT', 'POS', 'NEG',\n",
    "               'GIOIA', 'FIDUCIA', 'TRISTEZZA', 'RABBIA', 'PAURA', 'DISGUSTO',\n",
    "               'SORPRESA', 'TREPIDAZIONE', 'SARCASM', 'EMOTIONS',], axis=1, inplace=True)\n",
    "me_test.drop(['type', 'title', 'URL', 'comment', 'UNRELATED', 'NEUT', 'POS', 'NEG',\n",
    "               'GIOIA', 'FIDUCIA', 'TRISTEZZA', 'RABBIA', 'PAURA', 'DISGUSTO',\n",
    "               'SORPRESA', 'TREPIDAZIONE', 'SARCASM', 'EMOTIONS',], axis=1, inplace=True)\n",
    "\n",
    "print(\"Train: \", len(me_train) , np.unique(me_train.labels, return_counts=True)[1]/len(me_train))\n",
    "print(\"Test:\", len(me_test) , np.unique(me_test.labels, return_counts=True)[1]/len(me_test))\n",
    "\n",
    "print(\"\\nAverage comment lenght Train: \", np.mean(me_train[\"text\"].apply(len)))\n",
    "print(\"\\nAverage comment lenght Test: \", np.mean(me_test[\"text\"].apply(len)))\n",
    "me_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_me_train = pd.DataFrame({\"text\": me_train.text.apply(a.preprocess), \"idx\": me_train.index, \"labels\": me_train[\"labels\"].tolist()}) \n",
    "dataset_me_test  = pd.DataFrame({\"text\": me_test.text.apply(a.preprocess), \"idx\": me_test.index, \"labels\": me_test[\"labels\"].tolist()}) \n",
    "print(\"train: \", len(dataset_me_train[dataset_me_train.labels != 3]), np.unique(dataset_me_train[dataset_me_train.labels != 3].labels, return_counts=True)[1]/len(dataset_me_train[dataset_me_train.labels != 3]))\n",
    "print(\"Test: \", len(dataset_me_test[dataset_me_test.labels != 3]), np.unique(dataset_me_test[dataset_me_test.labels != 3].labels, return_counts=True)[1]/len(dataset_me_test[dataset_me_test.labels != 3]))\n",
    "\n",
    "lexicon_me = get_lexicon(dataset_me_test.text)\n",
    "print(\"MultiEmotions-IT lexicon  ∩ Feel-it lexicon: \", round(len(lexicon_me.intersection(lexicon_feel_it))/len(lexicon_me.union(lexicon_feel_it)),2))\n",
    "print(\"MultiEmotions-IT lexicon  ∩ Sentipolc lexicon: \", round(len(lexicon_me.intersection(lexicon_sentipolc))/len(lexicon_me.union(lexicon_sentipolc)),2))\n",
    "\n",
    "tmp = pd.concat([dataset_me_train[dataset_me_train.labels != 3], dataset_me_test[dataset_me_test.labels != 3]])\n",
    "print(\"All: \", len(tmp), np.unique(tmp.labels, return_counts=True)[1]/len(tmp))\n",
    "print(\"Size lexicon all: \", len(get_lexicon(tmp.text)))\n",
    "print(\"Avg num tokens: \", round(np.mean(tmp[\"text\"].apply(lambda x: len(x.split(\" \")))), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_me_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929489ee",
   "metadata": {},
   "source": [
    "### EvSent baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a0f6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script echo skipping\n",
    "examples = dataset_me_test.loc[dataset_me_test.labels != 3]\n",
    "\n",
    "preds , true , pbar = np.full(len(examples), -1) , np.full(len(examples), -1) , tqdm(total=len(examples))\n",
    "index , lock , p = 0 , Lock() , ThreadPool(processes=10)\n",
    "for text , label in zip(examples.text, examples.labels):\n",
    "    p.apply_async(worker, (label, text, pbar, preds, true, lock))\n",
    "p.close(); p.join()\n",
    "pbar.close()\n",
    "assert not np.any(preds == -1) and not np.any(true == -1)\n",
    "print(classification_report(true, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "##\n",
    "# No Neutrals\n",
    "# reassing labels so that no neutrals are present\n",
    "##\n",
    "print(\"\\n\\n No neutrals\\n\")\n",
    "evsent_remove_neutrals(true, preds)\n",
    "\n",
    "##\n",
    "# Stochastic MFC\n",
    "##\n",
    "print(\"\\n\\n SMFC\\n\")\n",
    "eval_mfc_baseline(dataset_me_train, dataset_me_test, keep_neutrals=True)\n",
    "\n",
    "print(\"\\n\\n SMFC\\n\")\n",
    "eval_mfc_baseline(dataset_me_train, dataset_me_test, keep_neutrals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b403e5ee",
   "metadata": {},
   "source": [
    "### AlBERTo MC/opt - MultiEmotions-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b25224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds2 , trues2, proba2 = eval_alberto(MyNetMC,      \"AlBERTo MC\", \"models/alberto_multiclass.pt\", dataset_me_test, \"MultiEmotions-IT\", return_preds=True, keep_neutrals=True, plot_confidence=False)\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", dataset_me_test, \"MultiEmotions-IT\", keep_neutrals=True, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_me_test[dataset_me_test.text.str.contains(\"é dedicata a tutti i ragazzi\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293fd286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(preds2)):\n",
    "    if preds2[i] == 2 and trues2[i] == 0:\n",
    "        print(\"\\n\\n\", dataset_me_test[dataset_me_test.labels != 3].text.tolist()[i], i)\n",
    "        print(\"Pred=\", preds2[i])\n",
    "        print(\"True=\", trues2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb76248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_alberto(MyNetMC,      \"AlBERTo MC\", \"models/alberto_multiclass.pt\", dataset_me_test, \"MultiEmotions-IT\", keep_neutrals=False, plot_confidence=False)\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", dataset_me_test, \"MultiEmotions-IT\", keep_neutrals=False, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843bb5c",
   "metadata": {},
   "source": [
    "### AlBERTo MC/opt fine-tuned - MultiEmotions-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a73e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds , trues , proba = eval_alberto(MyNetMC, \n",
    "                         \"AlBERTo MC\", \n",
    "                         \"models/alberto_multiclass.pt\", \n",
    "                         dataset_me_test, \n",
    "                         \"MultiEmotions-IT\", \n",
    "                         return_preds=True,\n",
    "                         keep_neutrals=True, \n",
    "                         plot_confidence=False, \n",
    "                         fine_tuning=True, \n",
    "                         dataset_train=dataset_me_train,\n",
    "                         logging=False,\n",
    "                         model_type=\"MC\"\n",
    "                        )\n",
    "\n",
    "eval_alberto(MyNetMCTuned, \n",
    "             \"AlBERTo MC opt\", \n",
    "             \"models/alberto_multiclass_tuned.pt\", \n",
    "             dataset_me_test, \n",
    "             \"MultiEmotions-IT\", \n",
    "             keep_neutrals=True, \n",
    "             plot_confidence=False, \n",
    "             fine_tuning=True, \n",
    "             dataset_train=dataset_me_train,\n",
    "             logging=False,\n",
    "             model_type=\"opt\"\n",
    "            );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe215e",
   "metadata": {},
   "source": [
    "##### inspect comments - neutrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a23c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_me_test[dataset_me_test.text.str.contains(\"anche se ascolto\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d002e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(preds)):\n",
    "    if preds[i] != trues[i] and trues[i] == 1:\n",
    "        print(\"\\n\\n\", dataset_me_test[dataset_me_test.labels != 3].text.tolist()[i], i)\n",
    "        print(\"Pred=\", preds[i])\n",
    "        print(\"True=\", trues[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe48f0e",
   "metadata": {},
   "source": [
    "### Feel-IT - MultiEmotions-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af36b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(dataset_me_test, keep_neutrals=True, dataset_name=\"MultiEmotions-IT\", plot_confidence=False)\n",
    "eval_feelit_model(dataset_me_test, keep_neutrals=False, dataset_name=\"MultiEmotions-IT\", plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80028fd9",
   "metadata": {},
   "source": [
    "### Feel-it MC fine tuned - MultiEmotions-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5de6aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(dataset_me_test, keep_neutrals=True, fine_tune=True, dataset_train=dataset_me_train, dataset_name=\"MultiEmotions-IT\", plot_confidence=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb40d2f",
   "metadata": {},
   "source": [
    "---\n",
    "# Coadapt sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb1ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_string2int(row):\n",
    "    if row[\"label\"] == \"neutral\":\n",
    "        return 1\n",
    "    elif row[\"label\"] == \"negative\":\n",
    "        return 0\n",
    "    elif row[\"label\"] == \"positive\":\n",
    "        return 2\n",
    "\n",
    "coadapt = pd.read_json(PATH + \"Coadapt/coadapt_sentiment.json\")\n",
    "coadapt = pd.DataFrame({\"text\": coadapt.text.apply(a.preprocess), \"idx\": coadapt.index, \"labels\": coadapt[[\"label\"]].apply(label_string2int, axis=1)}) \n",
    "\n",
    "print(\"Train: \", len(coadapt) , np.unique(coadapt.labels, return_counts=True)[1]/len(coadapt))\n",
    "print(\"\\nAverage text lenght Train: \", np.mean(coadapt[\"text\"].apply(len)))\n",
    "\n",
    "lexicon_coadapt = get_lexicon(coadapt.text)\n",
    "print(\"\\nCoadapt lexicon  ∩ Feel-it lexicon: \", round(len(lexicon_coadapt.intersection(lexicon_feel_it))/len(lexicon_coadapt.union(lexicon_feel_it)),2))\n",
    "print(\"Coadapt lexicon  ∩ Sentipolc lexicon: \", round(len(lexicon_coadapt.intersection(lexicon_sentipolc))/len(lexicon_coadapt.union(lexicon_sentipolc)),2))\n",
    "\n",
    "\n",
    "tmp = coadapt\n",
    "print(\"All: \", len(tmp), np.unique(tmp.labels, return_counts=True)[1]/len(tmp))\n",
    "print(\"Size lexicon all: \", len(get_lexicon(tmp.text)))\n",
    "print(\"Avg num tokens: \", round(np.mean(tmp[\"text\"].apply(lambda x: len(x.split(\" \")))), 2))\n",
    "\n",
    "coadapt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bba81d7",
   "metadata": {},
   "source": [
    "### EvSent baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec36830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%script echo skipping\n",
    "examples = coadapt\n",
    "\n",
    "preds , true , pbar = np.full(len(examples), -1) , np.full(len(examples), -1) , tqdm(total=len(examples))\n",
    "index , lock , p = 0 , Lock() , ThreadPool(processes=10)\n",
    "for text , label in zip(examples.text, examples.labels):\n",
    "    p.apply_async(worker, (label, text, pbar, preds, true, lock))\n",
    "p.close(); p.join()\n",
    "pbar.close()\n",
    "assert not np.any(preds == -1) and not np.any(true == -1)\n",
    "print(classification_report(true, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "##\n",
    "# No Neutrals\n",
    "# reassing labels so that no neutrals are present\n",
    "##\n",
    "print(\"\\n\\n No neutrals\\n\")\n",
    "evsent_remove_neutrals(true, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7479580",
   "metadata": {},
   "source": [
    "### AlBERTo MC/opt - Coadapt sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd383d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds2 , trues2 , proba2 = eval_alberto(MyNetMC, \"AlBERTo MC\", \"models/alberto_multiclass.pt\", coadapt, \"Coadapt\", keep_neutrals=True, return_preds=True, plot_confidence=False)\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", coadapt, \"Coadapt\", keep_neutrals=True, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295bbd0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(preds2)):\n",
    "    if preds2[i] == 0 and trues2[i] == 1:\n",
    "        print(\"\\n\\n\", coadapt.text.tolist()[i], i)\n",
    "        print(\"Pred=\", preds2[i])\n",
    "        print(\"True=\", trues2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ddc3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_alberto(MyNetMC, \"AlBERTo MC\", \"models/alberto_multiclass.pt\", coadapt, \"Coadapt\", keep_neutrals=False, return_preds=True, plot_confidence=False)\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", coadapt, \"Coadapt\", keep_neutrals=False, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88603410",
   "metadata": {},
   "source": [
    "##### inspect samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d59115",
   "metadata": {},
   "outputs": [],
   "source": [
    "coadapt[coadapt.text.str.contains(\"finito adesso primo colloqui\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf4c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(preds)):\n",
    "    if preds[i] != trues[i] and trues[i] == 2:\n",
    "        print(\"\\n\\n\", coadapt[coadapt.labels != 3].text.tolist()[i], i)\n",
    "        print(\"Pred=\", preds[i])\n",
    "        print(\"True=\", trues[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc538137",
   "metadata": {},
   "source": [
    "### Feel-it - Coadapt sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d7c972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(coadapt, keep_neutrals=False, dataset_name=\"Coadapt\", plot_confidence=False)\n",
    "eval_feelit_model(coadapt, keep_neutrals=True, dataset_name=\"Coadapt\", plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67624c99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# AriEmozione dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903555eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_train = pd.read_csv(PATH + \"Aria/ariaset_train.tsv\", sep='\\t', encoding=\"latin-1\", names=[\"names\", \"text\", \"labels\", \"confidence\", \"?\"])\n",
    "ari_train.drop([\"?\", \"names\"], axis=1, inplace=True)\n",
    "ari_train.dropna(inplace=True)\n",
    "\n",
    "ari_test = pd.read_csv(PATH + \"Aria/ariaset_test.tsv\", sep='\\t', encoding=\"latin-1\", names=[\"names\", \"text\", \"labels\", \"confidence\", \"?\"])\n",
    "ari_test.drop([\"?\", \"names\"], axis=1, inplace=True)\n",
    "ari_test.dropna(inplace=True)\n",
    "\n",
    "print(\"Train: \", len(ari_train) , np.unique(ari_train.labels, return_counts=True)[1]/len(ari_train))\n",
    "print(\"Test:\", len(ari_test) , np.unique(ari_test.labels, return_counts=True)[1]/len(ari_test))\n",
    "\n",
    "print(\"\\nAverage piece lenght Train: \", np.mean(ari_train[\"text\"].apply(len)))\n",
    "print(\"\\nAverage piece lenght Test: \", np.mean(ari_test[\"text\"].apply(len)))\n",
    "\n",
    "ari_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fadb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion2label(row):\n",
    "    if row[\"labels\"] in [\"Gioia\", \"Ammirazione\", \"Amore\"]:\n",
    "        return 2\n",
    "    elif row[\"labels\"] in [\"Rabbia\", \"Tristezza\", \"Paura\"]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "dataset_ari_train = pd.DataFrame({\"text\": ari_train.text.apply(a.preprocess), \"idx\": ari_train.index, \"labels\": ari_train[[\"labels\"]].apply(emotion2label, axis=1)}) \n",
    "dataset_ari_test = pd.DataFrame({\"text\": ari_test.text.apply(a.preprocess), \"idx\": ari_test.index, \"labels\": ari_test[[\"labels\"]].apply(emotion2label, axis=1)}) \n",
    "print(\"Train: \", len(dataset_ari_train) , np.unique(dataset_ari_train.labels, return_counts=True)[1]/len(dataset_ari_train))\n",
    "print(\"Test:\", len(dataset_ari_test) , np.unique(dataset_ari_test.labels, return_counts=True)[1]/len(dataset_ari_test))\n",
    "\n",
    "lexicon_ari = get_lexicon(dataset_ari_test.text)\n",
    "print(\"\\nAriEmozione lexicon  ∩ Feel-it lexicon: \", round(len(lexicon_ari.intersection(lexicon_feel_it))/len(lexicon_ari.union(lexicon_feel_it)),2))\n",
    "print(\"AriEmozione lexicon  ∩ Sentipolc lexicon: \", round(len(lexicon_ari.intersection(lexicon_sentipolc))/len(lexicon_ari.union(lexicon_sentipolc)),2))\n",
    "\n",
    "tmp = pd.concat([dataset_ari_train, dataset_ari_test])\n",
    "print(\"All: \", len(tmp), np.unique(tmp.labels, return_counts=True)[1]/len(tmp))\n",
    "print(\"Size lexicon all: \", len(get_lexicon(tmp.text)))\n",
    "print(\"Avg num tokens: \", round(np.mean(tmp[\"text\"].apply(lambda x: len(x.split(\" \")))), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ari_test.text[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e8168",
   "metadata": {},
   "source": [
    "### EvSent baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab38b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%script echo skipping\n",
    "examples = dataset_ari_test\n",
    "\n",
    "preds , true , pbar = np.full(len(examples), -1) , np.full(len(examples), -1) , tqdm(total=len(examples))\n",
    "index , lock , p = 0 , Lock() , ThreadPool(processes=10)\n",
    "for text , label in zip(examples.text, examples.labels):\n",
    "    p.apply_async(worker, (label, text, pbar, preds, true, lock))\n",
    "p.close(); p.join()\n",
    "pbar.close()\n",
    "assert not np.any(preds == -1) and not np.any(true == -1)\n",
    "print(classification_report(true, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "##\n",
    "# No Neutrals\n",
    "# reassing labels so that no neutrals are present\n",
    "##\n",
    "print(\"\\n\\n No neutrals\\n\")\n",
    "evsent_remove_neutrals(true, preds)\n",
    "\n",
    "##\n",
    "# Stochastic MFC\n",
    "##\n",
    "print(\"\\n\\n SMFC\\n\")\n",
    "eval_mfc_baseline(dataset_ari_train, dataset_ari_test, keep_neutrals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7434cb",
   "metadata": {},
   "source": [
    "### AlBERTo MC/opt - AriEmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563840f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds2 , trues2 , proba2 = eval_alberto(MyNetMC,      \"AlBERTo MC\", \"models/alberto_multiclass.pt\", dataset_ari_test, \"AriEmotions\", return_preds=True, keep_neutrals=True, plot_confidence=False)\n",
    "eval_alberto(MyNetMC,      \"AlBERTo MC\", \"models/alberto_multiclass.pt\", dataset_ari_test, \"AriEmotions\", return_preds=False, keep_neutrals=False, plot_confidence=False)\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", dataset_ari_test, \"AriEmotions\", keep_neutrals=True, plot_confidence=False)\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", dataset_ari_test, \"AriEmotions\", keep_neutrals=False, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3740af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(preds2)):\n",
    "    if preds2[i] == 1 and trues2[i] == 0 or \"conoscerai chi sono ti pentirai del dono ma sarà tardi allor\" in dataset_ari_test.text.tolist()[i]:\n",
    "        print(\"\\n\\n\", dataset_ari_test.text.tolist()[i], i)\n",
    "        print(\"Pred=\", preds2[i])\n",
    "        print(\"True=\", trues2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad71cf",
   "metadata": {},
   "source": [
    "### AlBERTo MC/opt fine tuned - AriEmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d78abda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_alberto(MyNetMC, \n",
    "             \"AlBERTo MC\", \n",
    "             \"models/alberto_multiclass.pt\", \n",
    "             dataset_ari_test, \n",
    "             \"AriEmotions\", \n",
    "             keep_neutrals=True, \n",
    "             plot_confidence=False, \n",
    "             fine_tuning=True, \n",
    "             dataset_train=dataset_ari_train,\n",
    "             logging=False,\n",
    "             model_type=\"MC\"\n",
    "            )\n",
    "\n",
    "eval_alberto(MyNetMCTuned, \n",
    "             \"AlBERTo MC opt\", \n",
    "             \"models/alberto_multiclass_tuned.pt\", \n",
    "             dataset_ari_test, \n",
    "             \"AriEmotions\", \n",
    "             keep_neutrals=True, \n",
    "             plot_confidence=False, \n",
    "             fine_tuning=True, \n",
    "             dataset_train=dataset_ari_train,\n",
    "             logging=False,\n",
    "             model_type=\"opt\"\n",
    "            );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f5f7a",
   "metadata": {},
   "source": [
    "### Feel-it model - AriEmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c5092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(dataset_ari_test, keep_neutrals=True, dataset_name=\"AriEmotions\", plot_confidence=False)\n",
    "eval_feelit_model(dataset_ari_test, keep_neutrals=False, dataset_name=\"AriEmotions\", plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37da7b2",
   "metadata": {},
   "source": [
    "### Feel-it MC fine tuned - AriEmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b9e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(dataset_ari_test, keep_neutrals=True, fine_tune=True, dataset_train=dataset_ari_train, dataset_name=\"AriEmotions\", plot_confidence=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e609584",
   "metadata": {},
   "source": [
    "---\n",
    "# Trip-maml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segments2text(row):\n",
    "    return \" \".join([ e.split(\"/\")[0] for sub_segment in row[\"segments\"] for e in sub_segment.split(\" \")]) \n",
    "\n",
    "def rating2sentiment(row):\n",
    "    \"\"\"\n",
    "        Returns the polarity depending on the rating of the review\n",
    "    \"\"\"\n",
    "    if row[\"ratingOverall\"] == 3:\n",
    "        return 1\n",
    "    elif row[\"ratingOverall\"] > 3:\n",
    "        return 2\n",
    "    elif row[\"ratingOverall\"] < 3:\n",
    "        return 0\n",
    "\n",
    "trip_train = pd.read_json(PATH + \"Trip-maml/italian_training.json\", lines=True)\n",
    "dataset_trip_train = pd.DataFrame({\"text\": trip_train[[\"segments\"]].apply(segments2text, axis=1), \"idx\": trip_train.index, \"labels\": trip_train[[\"ratingOverall\"]].apply(rating2sentiment, axis=1)}) \n",
    "\n",
    "trip_test = pd.read_json(PATH + \"Trip-maml/italian_test.json\", lines=True)\n",
    "dataset_trip_test = pd.DataFrame({\"text\": trip_test[[\"segments\"]].apply(segments2text, axis=1), \"idx\": trip_test.index, \"labels\": trip_test[[\"ratingOverall\"]].apply(rating2sentiment, axis=1)}) \n",
    "\n",
    "print(\"Train: \", len(dataset_trip_train) , np.unique(dataset_trip_train.labels, return_counts=True)[1]/len(dataset_trip_train))\n",
    "print(\"Test:  \", len(dataset_trip_test) , np.unique(dataset_trip_test.labels, return_counts=True)[1]/len(dataset_trip_test))\n",
    "\n",
    "print(\"\\nAverage piece lenght Train: \", np.mean(dataset_trip_train[\"text\"].apply(len)))\n",
    "print(\"\\nAverage piece lenght Test: \", np.mean(dataset_trip_test[\"text\"].apply(len)))\n",
    "\n",
    "lexicon_trip = get_lexicon(dataset_trip_test.text)\n",
    "print(\"\\nTrip-maml lexicon  ∩ Feel-it lexicon: \", round(len(lexicon_trip.intersection(lexicon_feel_it))/len(lexicon_trip.union(lexicon_feel_it)),2))\n",
    "print(\"Trip-maml lexicon  ∩ Sentipolc lexicon: \", round(len(lexicon_trip.intersection(lexicon_sentipolc))/len(lexicon_trip.union(lexicon_sentipolc)),2))\n",
    "\n",
    "tmp = pd.concat([dataset_trip_train, dataset_trip_test])\n",
    "print(\"All: \", len(tmp), np.unique(tmp.labels, return_counts=True)[1]/len(tmp))\n",
    "print(\"Size lexicon all: \", len(get_lexicon(tmp.text)))\n",
    "print(\"Avg num tokens: \", round(np.mean(tmp[\"text\"].apply(lambda x: len(x.split(\" \")))), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6d133d",
   "metadata": {},
   "source": [
    "### EvSent baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d5a966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%script echo skipping\n",
    "examples = dataset_trip_test[dataset_trip_test.labels != 3]\n",
    "\n",
    "preds , true , pbar = np.full(len(examples), -1) , np.full(len(examples), -1) , tqdm(total=len(examples))\n",
    "index , lock , p = 0 , Lock() , ThreadPool(processes=10)\n",
    "for text , label in zip(examples.text, examples.labels):\n",
    "    p.apply_async(worker, (label, text, pbar, preds, true, lock))\n",
    "p.close(); p.join()\n",
    "pbar.close()\n",
    "assert not np.any(preds == -1) and not np.any(true == -1)\n",
    "print(classification_report(true, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "##\n",
    "# No Neutrals\n",
    "# reassing labels so that no neutrals are present\n",
    "##\n",
    "print(\"\\n\\n No neutrals\\n\")\n",
    "evsent_remove_neutrals(true, preds)\n",
    "\n",
    "##\n",
    "# Stochastic MFC\n",
    "##\n",
    "print(\"\\n\\n SMFC\\n\")\n",
    "eval_mfc_baseline(dataset_trip_train, dataset_trip_test, keep_neutrals=True)\n",
    "\n",
    "print(\"\\n\\n SMFC no neutrals\\n\")\n",
    "eval_mfc_baseline(dataset_trip_train, dataset_trip_test, keep_neutrals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d1bde",
   "metadata": {},
   "source": [
    "### AlBERTo MC/opt - Trip-maml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5991c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds2 , trues2 , proba2 = eval_alberto(MyNetMC,      \"AlBERTo MC\", \"models/alberto_multiclass.pt\", dataset_trip_test, \"Trip-maml\", return_preds=True, keep_neutrals=True, plot_confidence=False)\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", dataset_trip_test, \"Trip-maml\", keep_neutrals=True, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ebe26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(preds2)):\n",
    "    if preds2[i] == 2 and trues2[i] == 1:\n",
    "        print(\"\\n\\n\", dataset_trip_test[dataset_trip_test.labels != 3].text.tolist()[i], i)\n",
    "        print(\"Pred=\", preds2[i])\n",
    "        print(\"True=\", trues2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4347c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_alberto(MyNetMC,      \"AlBERTo MC\", \"models/alberto_multiclass.pt\", dataset_trip_test, \"Trip-maml\", keep_neutrals=False, plot_confidence=False)\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", dataset_trip_test, \"Trip-maml\", keep_neutrals=False, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a0bef",
   "metadata": {},
   "source": [
    "### AlBERTo MC/opt fine tuned - Trip-maml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377715f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds , trues , proba = eval_alberto(MyNetMC, \n",
    "                         \"AlBERTo MC\", \n",
    "                         \"models/alberto_multiclass.pt\", \n",
    "                         dataset_trip_test, \n",
    "                         \"Trip-maml\", \n",
    "                         keep_neutrals=True, \n",
    "                         return_preds=True,\n",
    "                         plot_confidence=False, \n",
    "                         fine_tuning=True, \n",
    "                         dataset_train=dataset_trip_train,\n",
    "                         logging=False,\n",
    "                         model_type=\"MC\"\n",
    "                        )\n",
    "\n",
    "eval_alberto(MyNetMCTuned, \n",
    "             \"AlBERTo MC opt\", \n",
    "             \"models/alberto_multiclass_tuned.pt\", \n",
    "             dataset_trip_test, \n",
    "             \"Trip-maml\", \n",
    "             keep_neutrals=True, \n",
    "             plot_confidence=False, \n",
    "             fine_tuning=True, \n",
    "             dataset_train=dataset_trip_train,\n",
    "             logging=False,\n",
    "             model_type=\"opt\"\n",
    "            );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8b9b0",
   "metadata": {},
   "source": [
    "##### inspect samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_trip_test[dataset_trip_test.text.str.contains(\"Leggendo le rece\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431afd5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(preds)):\n",
    "    if preds[i] != trues[i] and trues[i] == 0:\n",
    "        print(\"\\n\\n\", dataset_trip_test.text.tolist()[i], i)\n",
    "        print(\"Pred=\", preds[i])\n",
    "        print(\"True=\", trues[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b57b2",
   "metadata": {},
   "source": [
    "### Feel-it model - Trip-maml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb984a82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(dataset_trip_test, keep_neutrals=False, dataset_name=\"Trip-maml\", plot_confidence=False)\n",
    "eval_feelit_model(dataset_trip_test, keep_neutrals=True, dataset_name=\"Trip-maml\", plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad4c2d",
   "metadata": {},
   "source": [
    "### Feel-it MC fine tuned - Trip-MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5fdf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(dataset_trip_test, keep_neutrals=True, fine_tune=True, dataset_train=dataset_trip_train, dataset_name=\"Trip-maml\", plot_confidence=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea8573",
   "metadata": {},
   "source": [
    "# Coadapt valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_string2int(row):\n",
    "    if row[\"label\"] == \"neutral\":\n",
    "        return 1\n",
    "    elif row[\"label\"] == \"negative\":\n",
    "        return 0\n",
    "    elif row[\"label\"] == \"positive\":\n",
    "        return 2\n",
    "\n",
    "coadapt_v_train = pd.read_json(PATH + \"Coadapt_valence/train_set.json\")\n",
    "coadapt_v_train = pd.DataFrame({\"text\": coadapt_v_train.text.apply(a.preprocess), \"idx\": coadapt_v_train.index, \"labels\": coadapt_v_train[[\"label\"]].apply(label_string2int, axis=1)}) \n",
    "print(\"Train: \", len(coadapt_v_train) , np.unique(coadapt_v_train.labels, return_counts=True)[1]/len(coadapt_v_train))\n",
    "\n",
    "coadapt_v_A = pd.read_json(PATH + \"Coadapt_valence/partition_A.json\")\n",
    "coadapt_v_A = pd.DataFrame({\"text\": coadapt_v_A.text.apply(a.preprocess), \"idx\": coadapt_v_A.index, \"labels\": coadapt_v_A[[\"label\"]].apply(label_string2int, axis=1)}) \n",
    "\n",
    "coadapt_v_B = pd.read_json(PATH + \"Coadapt_valence/partition_B.json\")\n",
    "coadapt_v_B = pd.DataFrame({\"text\": coadapt_v_B.text.apply(a.preprocess), \"idx\": coadapt_v_B.index, \"labels\": coadapt_v_B[[\"label\"]].apply(label_string2int, axis=1)}) \n",
    "\n",
    "coadapt_v_C = pd.read_json(PATH + \"Coadapt_valence/partition_C.json\")\n",
    "coadapt_v_C = pd.DataFrame({\"text\": coadapt_v_C.text.apply(a.preprocess), \"idx\": coadapt_v_C.index, \"labels\": coadapt_v_C[[\"label\"]].apply(label_string2int, axis=1)}) \n",
    "\n",
    "coadapt_v_D = pd.read_json(PATH + \"Coadapt_valence/partition_D.json\")\n",
    "coadapt_v_D = pd.DataFrame({\"text\": coadapt_v_D.text.apply(a.preprocess), \"idx\": coadapt_v_D.index, \"labels\": coadapt_v_D[[\"label\"]].apply(label_string2int, axis=1)}) \n",
    "\n",
    "\n",
    "tmp = pd.concat([coadapt_v_train, coadapt_v_A, coadapt_v_B, coadapt_v_C, coadapt_v_D])\n",
    "print(\"All: \", len(tmp), np.unique(tmp.labels, return_counts=True)[1]/len(tmp))\n",
    "print(\"Size lexicon all: \", len(get_lexicon(tmp.text)))\n",
    "print(\"Avg num tokens: \", round(np.mean(tmp[\"text\"].apply(lambda x: len(x.split(\" \")))), 2))\n",
    "\n",
    "coadapt_v_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1d69f",
   "metadata": {},
   "source": [
    "### EvSent baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71ff76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "examples = pd.concat([coadapt_v_C, coadapt_v_D]) \n",
    "\n",
    "preds , true , pbar = np.full(len(examples), -1) , np.full(len(examples), -1) , tqdm(total=len(examples))\n",
    "index , lock , p = 0 , Lock() , ThreadPool(processes=10)\n",
    "for text , label in zip(examples.text, examples.labels):\n",
    "    p.apply_async(worker, (label, text, pbar, preds, true, lock))\n",
    "p.close(); p.join()\n",
    "pbar.close()\n",
    "assert not np.any(preds == -1) and not np.any(true == -1)\n",
    "print(classification_report(true, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "##\n",
    "# No Neutrals\n",
    "# reassing labels so that no neutrals are present\n",
    "##\n",
    "print(\"\\n\\n No neutrals\\n\")\n",
    "evsent_remove_neutrals(true, preds)\n",
    "\n",
    "##\n",
    "# Stochastic MFC\n",
    "##\n",
    "print(\"\\n\\n SMFC\\n\")\n",
    "eval_mfc_baseline(coadapt_v_train, pd.concat([coadapt_v_C, coadapt_v_D]), keep_neutrals=True)\n",
    "\n",
    "print(\"\\n\\n SMFC no neutrals\\n\")\n",
    "eval_mfc_baseline(coadapt_v_train, pd.concat([coadapt_v_C, coadapt_v_D]), keep_neutrals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18bab00",
   "metadata": {},
   "source": [
    "### AlBERTo MC/opt - Coadapt valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa1106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds2 , trues2 , proba2 = eval_alberto(MyNetMC,      \"AlBERTo MC\", \"models/alberto_multiclass.pt\", pd.concat([coadapt_v_C, coadapt_v_D]), \"Coadapt valence\", return_preds=True, keep_neutrals=True, plot_confidence=False)\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", pd.concat([coadapt_v_C, coadapt_v_D]), \"Coadapt valence\", keep_neutrals=True, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212c5c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_alberto(MyNetMC,      \"AlBERTo MC\",           \"models/alberto_multiclass.pt\", pd.concat([coadapt_v_C, coadapt_v_D]), \"Coadapt valence\", return_preds=True, keep_neutrals=False, plot_confidence=False)\n",
    "eval_alberto(MyNetMCTuned, \"AlBERTo MC opt\", \"models/alberto_multiclass_tuned.pt\", pd.concat([coadapt_v_C, coadapt_v_D]), \"Coadapt valence\", keep_neutrals=False, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed36a50",
   "metadata": {},
   "source": [
    "### AlBERTo MC/opt fine tuned - Coadapt valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562bbf15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_alberto(MyNetMC, \n",
    "                         \"AlBERTo MC\", \n",
    "                         \"models/alberto_multiclass.pt\", \n",
    "                         pd.concat([coadapt_v_C, coadapt_v_D]), \n",
    "                         \"Coadapt valence\", \n",
    "                         keep_neutrals=True, \n",
    "                         return_preds=True,\n",
    "                         plot_confidence=False, \n",
    "                         fine_tuning=True, \n",
    "                         dataset_train=coadapt_v_train,\n",
    "                         logging=False,\n",
    "                         model_type=\"MC\"\n",
    "                        )\n",
    "\n",
    "eval_alberto(MyNetMCTuned, \n",
    "             \"AlBERTo MC opt\", \n",
    "             \"models/alberto_multiclass_tuned.pt\", \n",
    "             pd.concat([coadapt_v_C, coadapt_v_D]), \n",
    "             \"Coadapt valence\", \n",
    "             keep_neutrals=True, \n",
    "             plot_confidence=False, \n",
    "             fine_tuning=True, \n",
    "             dataset_train=coadapt_v_train,\n",
    "             logging=False,\n",
    "             model_type=\"opt\"\n",
    "            );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13039dbe",
   "metadata": {},
   "source": [
    "### Feel-it model - Coadapt valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba6b327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(pd.concat([coadapt_v_C, coadapt_v_D]), keep_neutrals=False, dataset_name=\"Coadapt valence\", plot_confidence=False)\n",
    "eval_feelit_model(pd.concat([coadapt_v_C, coadapt_v_D]), keep_neutrals=True, dataset_name=\"Coadapt valence\", plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce93e3a",
   "metadata": {},
   "source": [
    "### Feel-it model fine tuned - Coadapt valence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37445078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(pd.concat([coadapt_v_C, coadapt_v_D]), keep_neutrals=True, fine_tune=True, dataset_train=coadapt_v_train, dataset_name=\"Coadapt valence\", plot_confidence=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c34fa",
   "metadata": {},
   "source": [
    "---\n",
    "# Training on all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc24a71",
   "metadata": {},
   "source": [
    "##### Feel-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all_feel_it = pd.concat([coadapt_v_train, dataset_sentipolc_train, dataset_amazon_train, dataset_ari_train, dataset_trip_train, dataset_me_train])\n",
    "print(\"All train datasets: \", len(dataset_all_feel_it[dataset_all_feel_it.labels != 3]), np.unique(dataset_all_feel_it[dataset_all_feel_it.labels != 3].labels, return_counts=True)[1]/len(dataset_all_feel_it[dataset_all_feel_it.labels != 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62afc619",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = eval_feelit_model(dataset_trip_test, keep_neutrals=True, fine_tune=True, dataset_train=dataset_all_feel_it, dataset_name=\"Trip-maml\", plot_confidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e453b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_feelit_model(dataset_trip_test,     keep_neutrals=True, fine_tune=False, model_i=model, dataset_name=\"Trip-maml\", plot_confidence=False)\n",
    "eval_feelit_model(coadapt,               keep_neutrals=True, fine_tune=False, model_i=model, dataset_name=\"Coadapt\", plot_confidence=False)\n",
    "eval_feelit_model(dataset_feel_it,       keep_neutrals=True, fine_tune=False, model_i=model, dataset_name=\"Feel-it\", plot_confidence=False)\n",
    "eval_feelit_model(dataset_ari_test,      keep_neutrals=True, fine_tune=False, model_i=model, dataset_name=\"AriEmotions\", plot_confidence=False)\n",
    "eval_feelit_model(dataset_sentipolc_test,keep_neutrals=True, fine_tune=False, model_i=model, dataset_name=\"SentiPolc16\", plot_confidence=False)\n",
    "eval_feelit_model(dataset_me_test,       keep_neutrals=True, fine_tune=False, model_i=model, dataset_name=\"MultiEMotions\", plot_confidence=False)\n",
    "eval_feelit_model(dataset_amazon_test,   keep_neutrals=True, fine_tune=False, model_i=model, dataset_name=\"Amazon reviews\", plot_confidence=False)\n",
    "eval_feelit_model(pd.concat([coadapt_v_C, coadapt_v_D]),   keep_neutrals=True, fine_tune=False, model_i=model, dataset_name=\"Coadapt valence\", plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c2945",
   "metadata": {},
   "source": [
    "###### AlBERTo MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c57c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all_alberto = pd.concat([coadapt_v_train, dataset_amazon_train, dataset_ari_train, dataset_trip_train, dataset_me_train])\n",
    "print(\"All train datasets: \", len(dataset_all_alberto[dataset_all_alberto.labels != 3]), np.unique(dataset_all_alberto[dataset_all_alberto.labels != 3].labels, return_counts=True)[1]/len(dataset_all_alberto[dataset_all_alberto.labels != 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17987e66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = eval_alberto(MyNetMC, \"AlBERTo MC\", \"models/alberto_multiclass.pt\", dataset_sentipolc_test, \"Sentipolc16\", fine_tuning=True, dataset_train=dataset_all_alberto, model_type=\"MC\", logging=True, keep_neutrals=True, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ceac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_alberto(None, \"AlBERTo MC fine tuned on all datasets\", None, dataset_sentipolc_test, \"Sentipolc16\", fine_tuning=False, model_init=model, keep_neutrals=True, plot_confidence=False)\n",
    "eval_alberto(None, \"AlBERTo MC fine tuned on all datasets\", None, dataset_trip_test, \"Trip-maml\", fine_tuning=False, model_init=model, keep_neutrals=True, plot_confidence=False)\n",
    "#eval_alberto(None, \"AlBERTo MC fine tuned on all datasets\", None, coadapt, \"Coadapt\", fine_tuning=False, model_init=model, keep_neutrals=True, plot_confidence=False)\n",
    "eval_alberto(None, \"AlBERTo MC fine tuned on all datasets\", None, dataset_feel_it, \"Feel-it\", fine_tuning=False, model_init=model, keep_neutrals=True, plot_confidence=False)\n",
    "eval_alberto(None, \"AlBERTo MC fine tuned on all datasets\", None, dataset_ari_test, \"AriEmotions\", fine_tuning=False, model_init=model, keep_neutrals=True, plot_confidence=False)\n",
    "eval_alberto(None, \"AlBERTo MC fine tuned on all datasets\", None, dataset_me_test, \"MultiEmotions\", fine_tuning=False, model_init=model, keep_neutrals=True, plot_confidence=False)\n",
    "eval_alberto(None, \"AlBERTo MC fine tuned on all datasets\", None, dataset_amazon_test, \"Amazon reviews\", fine_tuning=False, model_init=model, keep_neutrals=True, plot_confidence=False)\n",
    "eval_alberto(None, \"AlBERTo MC fine tuned on all datasets\", None, pd.concat([coadapt_v_C, coadapt_v_D]), \"Coadapt valence\", fine_tuning=False, model_init=model, keep_neutrals=True, plot_confidence=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496a0ac",
   "metadata": {},
   "source": [
    "---\n",
    "#### Further error analysis\n",
    "Use EvSent to compute the number of words with either a positive or negative meaning, in order to spot samples clearly positive or clearly negative.\n",
    "Then use AlBERTo or Feel-IT to see the its behaviour on samples that have no clear polarity orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b8ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "a_file = open(PATH + \"sentiment_lexicon.pkl\", \"rb\")\n",
    "sentiment_lexicon = pickle.load(a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad797ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokenized_token(sa):\n",
    "    ret = []\n",
    "    for token in sa:\n",
    "        if token in (\"<\", \">\", \"/\"):\n",
    "            continue\n",
    "        if token[:2] == \"##\":\n",
    "            ret[-1] = ret[-1] + token[2:]\n",
    "        else:\n",
    "            ret.append(token)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def add_to_sentiment_lexicon(dataset):\n",
    "    for sub_tokenized_sample in tqdm(dataset.text.apply(lambda x: tok.tokenize(x))):\n",
    "        tokenized_sample = join_tokenized_token(sub_tokenized_sample)\n",
    "        for token in tokenized_sample:\n",
    "            if token not in sentiment_lexicon:\n",
    "                polarity = evSent(token)\n",
    "                sentiment_lexicon[token] = polarity\n",
    "\n",
    "#build the sentiment lexicon\n",
    "add_to_sentiment_lexicon(dataset_sentipolc[dataset_sentipolc.labels != 3])\n",
    "add_to_sentiment_lexicon(dataset_feel_it)\n",
    "add_to_sentiment_lexicon(dataset_amazon_test)\n",
    "add_to_sentiment_lexicon(dataset_me_test)\n",
    "add_to_sentiment_lexicon(coadapt)\n",
    "add_to_sentiment_lexicon(dataset_ari_test)\n",
    "add_to_sentiment_lexicon(dataset_trip_test)\n",
    "\n",
    "# a_file = open(PATH + \"sentiment_lexicon.pkl\", \"wb\")\n",
    "# pickle.dump(sentiment_lexicon, a_file)\n",
    "# a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(dataset):\n",
    "    ret , ret2 = [] , []\n",
    "    for sub_tokenized_sample in tqdm(dataset.text.apply(lambda x: tok.tokenize(x))):\n",
    "        tokenized_sample = join_tokenized_token(sub_tokenized_sample)\n",
    "        score = 0\n",
    "        has_neg , has_pos = False , False\n",
    "        for token in tokenized_sample:\n",
    "            if sentiment_lexicon[token] == 2:\n",
    "                score += 1\n",
    "                has_pos = True\n",
    "            elif sentiment_lexicon[token] == 0:\n",
    "                score -= 1 \n",
    "                has_neg = True\n",
    "        ret.append(score)\n",
    "        ret2.append(has_neg and has_pos)\n",
    "    return np.array(ret) , np.array(ret2)\n",
    "\n",
    "scores , has_boths = score_dataset(dataset_sentipolc[dataset_sentipolc.labels != 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , text in enumerate(dataset_sentipolc[dataset_sentipolc.labels != 3].text.tolist()):\n",
    "    print(scores[i], has_boths[i] , dataset_sentipolc[dataset_sentipolc.labels != 3].labels.tolist()[i], \" --> \" ,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy()[has_boths], return_counts=True))\n",
    "print(np.unique(dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy()[scores == 0], return_counts=True))\n",
    "print(np.unique(dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy()[np.logical_and(scores == 0, has_boths)], return_counts=True))\n",
    "print(np.unique(dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy()[np.logical_and(scores == 0, ~has_boths)], return_counts=True))\n",
    "print(np.unique(dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy()[scores >= 2], return_counts=True))\n",
    "print(np.unique(dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy()[scores >= 3], return_counts=True))\n",
    "print(np.unique(dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy()[scores <= -1], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds , trues , proba = eval_alberto(MyNetMC, \"AlBERTo MC\", \"models/alberto_multiclass.pt\", dataset_sentipolc, \"Sentipolc16\", return_preds=True, keep_neutrals=True, plot_confidence=False)\n",
    "assert np.all(trues == dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Violin(y=dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy()[has_boths], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='lightseagreen', opacity=0.6,\n",
    "                               x0='Has both'))\n",
    "fig.add_trace(go.Violin(y=preds[has_boths], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='orange', opacity=0.6,\n",
    "                               x0='Pred has both'))\n",
    "\n",
    "fig.add_trace(go.Violin(y=dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy()[scores == 0], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='lightseagreen', opacity=0.6,\n",
    "                               x0='Neutral lexicon score'))\n",
    "fig.add_trace(go.Violin(y=preds[scores == 0], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='orange', opacity=0.6,\n",
    "                               x0='Pred neutral lexicon score'))\n",
    "\n",
    "fig.add_trace(go.Violin(y=dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy()[np.logical_and(scores == 0, has_boths)], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='lightseagreen', opacity=0.6,\n",
    "                               x0='Neutral lexicon score + has both'))\n",
    "fig.add_trace(go.Violin(y=preds[np.logical_and(scores == 0, has_boths)], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='orange', opacity=0.6,\n",
    "                               x0='Pred neutral lexicon score + has both'))\n",
    "\n",
    "fig.add_trace(go.Violin(y=dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy()[np.logical_and(scores == 0, ~has_boths)], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='lightseagreen', opacity=0.6,\n",
    "                               x0='Neutral lexicon score + not has both'))\n",
    "fig.add_trace(go.Violin(y=preds[np.logical_and(scores == 0, ~has_boths)], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='orange', opacity=0.6,\n",
    "                               x0='Pred neutral lexicon score + not has both'))\n",
    "\n",
    "fig.add_trace(go.Violin(y=dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy()[scores >= 2], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='lightseagreen', opacity=0.6,\n",
    "                               x0='Score >= 2'))\n",
    "fig.add_trace(go.Violin(y=preds[scores >= 2], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='orange', opacity=0.6,\n",
    "                               x0='Pred score >= 2'))\n",
    "\n",
    "fig.add_trace(go.Violin(y=dataset_sentipolc[dataset_sentipolc.labels != 3].labels.to_numpy()[scores <= -1], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='lightseagreen', opacity=0.6,\n",
    "                               x0='Score <= -1'))\n",
    "fig.add_trace(go.Violin(y=preds[scores <= -1], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='orange', opacity=0.6,\n",
    "                               x0='Pred score <= -1'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of sentiment lexicon's scores and model's predictions\",\n",
    "    yaxis_zeroline=False,\n",
    "    showlegend=False,\n",
    "    yaxis = dict(\n",
    "            tickmode = 'array',\n",
    "            tickvals = [0, 1, 2, 3],\n",
    "            ticktext = ['Negative', 'Neutral', 'Positive', 'Mixed']\n",
    "        )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bbe205",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Violin(y=scores[np.logical_and(trues == 2, preds == 2)], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='orange', opacity=0.6,\n",
    "                               x0='True positives'))\n",
    "\n",
    "fig.add_trace(go.Violin(y=scores[np.logical_and(trues == 0, preds == 0)], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='orange', opacity=0.6,\n",
    "                               x0='True negatives'))\n",
    "\n",
    "fig.add_trace(go.Violin(y=scores[np.logical_and(trues == 1, preds == 1)], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='orange', opacity=0.6,\n",
    "                               x0='True neutrals'))\n",
    "\n",
    "fig.add_trace(go.Violin(y=scores[np.logical_and(trues != 2, preds == 2)], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='lightseagreen', opacity=0.6,\n",
    "                               x0='False positives'))\n",
    "\n",
    "fig.add_trace(go.Violin(y=scores[np.logical_and(trues != 0, preds == 0)], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='lightseagreen', opacity=0.6,\n",
    "                               x0='False negatives'))\n",
    "\n",
    "fig.add_trace(go.Violin(y=scores[np.logical_and(trues != 1, preds == 1)], line_color='black',\n",
    "                               meanline_visible=False, fillcolor='lightseagreen', opacity=0.6,\n",
    "                               x0='False neutrals'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Distribution of sentiment lexicon's scores and model's predictions\",\n",
    "    yaxis_zeroline=False,\n",
    "    showlegend=False,\n",
    "    yaxis = dict(\n",
    "            tickmode = 'array',\n",
    "            tickvals = [-2, -1, 0, 1, 2, 3, 4, 5]\n",
    "        )\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
