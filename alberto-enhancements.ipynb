{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ekphrasis\n!pip install tqdm boto3 requests regex sentencepiece sacremoses pytorch-transformers\n!pip install transformers \n!pip install datasets\n!pip install optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-13T14:27:36.342333Z","iopub.status.idle":"2021-07-13T14:27:36.342727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env WANDB_PROJECT=nlu_sentiment_analysis\n!wandb login 2cad8a8279143c69ce071f54bf37c1f5a5f4e5ff\nimport wandb\n\nfrom ekphrasis.classes.preprocessor import TextPreProcessor\nfrom ekphrasis.classes.tokenizer import SocialTokenizer\nfrom ekphrasis.dicts.emoticons import emoticons\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport itertools\nimport requests, re, string, datetime, copy\nfrom functools import partial\n\nimport torch\nimport torchvision.transforms as T, torch.nn.functional as F, torch.nn as nn\n\nfrom datasets import Dataset\nfrom transformers import TrainingArguments, EarlyStoppingCallback, TrainerCallback\nfrom transformers import Trainer\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\nfrom sklearn.model_selection import train_test_split\n\nPATH = \"../input/sentipolc2016/\"","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:27:36.343826Z","iopub.status.idle":"2021-07-13T14:27:36.344386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(PATH + \"training_set_sentipolc16.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:27:36.345527Z","iopub.status.idle":"2021-07-13T14:27:36.346183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file1 = open(PATH + \"test_set_sentipolc16_gold2000.csv\", 'r') \nLines = file1.readlines()\n \ntest = []\nfor line in Lines:\n  arr = line.split(\"\\\",\")\n  if len(arr) != 9:\n    arr[8] = arr[8] + arr[9]  #to account for tweets containing the delimiter charachter that would create more splits than needed\n    del arr[9:]\n  for i in range(8):\n    arr[i] = int(arr[i].strip(\"\\\"\"))\n  test.append(arr)\n\ntest = pd.DataFrame(test, columns=train.columns)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:27:36.347368Z","iopub.status.idle":"2021-07-13T14:27:36.348021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Tokenization classes for Italian AlBERTo models.\"\"\"\nimport collections\nimport os\n\n#from transformers import BertTokenizer, WordpieceTokenizer\n\ndef separate2united_labels(row):\n  \"\"\"\n  Return a single scalar integer label associated to the polarity of the tweet.\n\n  Negative -> 0\n  Neutral  -> 1\n  Positive -> 2\n  Mixed    -> 3\n  \"\"\"\n  if row[\"opos\"] == 0 and row[\"oneg\"] == 0:\n    return 1\n  elif row[\"oneg\"] == 0 and row[\"opos\"] == 1:\n    return 2\n  elif row[\"oneg\"] == 1 and row[\"opos\"] == 0:\n    return 0\n  else:\n    return 3\n\ndef load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        tokens = reader.readlines()\n    for index, token in enumerate(tokens):\n        token = token.rstrip(\"\\n\")\n        vocab[token] = index\n    return vocab\n\ntext_processor = TextPreProcessor(\n    # terms that will be normalized\n    normalize=['url', 'email', 'user', 'percent', 'money', 'phone', 'time', 'date', 'number'],\n    # terms that will be annotated\n    annotate={\"hashtag\"},\n    fix_html=True,  # fix HTML tokens\n\n    unpack_hashtags=True,  # perform word segmentation on hashtags\n\n    # select a tokenizer. You can use SocialTokenizer, or pass your own\n    # the tokenizer, should take as input a string and return a list of tokens\n    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n    dicts=[emoticons]\n)\n\nclass AlBERTo_Preprocessing(object):\n    def __init__(self, do_lower_case=True, **kwargs):\n        self.do_lower_case = do_lower_case\n\n    def preprocess(self, text):\n        if self.do_lower_case:\n            text = text.lower()\n        text = str(\" \".join(text_processor.pre_process_doc(text)))\n        text = re.sub(r'[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]', ' ', text)\n        text = re.sub(r'\\s+', ' ', text)\n        text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n        text = re.sub(r'^\\s', '', text)\n        text = re.sub(r'\\s$', '', text)\n        return text\n\na = AlBERTo_Preprocessing(do_lower_case=True)\ns = \"#IlGOverno presenta le linee guida sulla scuola #labuonascuola - http://t.co/SYS1T9QmQN\"\nb = a.preprocess(s)\nprint(b)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:27:36.349260Z","iopub.status.idle":"2021-07-13T14:27:36.349871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def k_steps_evidence(num_iterations, early_stopping_patience, training_args, net, training_opos, training_oneg, testing_opos, testing_oneg, validating_opos, validating_oneg):\n    np.random.seed(0)\n    torch.manual_seed(0)\n\n    keys = [\"eval_loss\", \"eval_accuracy\", \"eval_f1\", \"eval_precision\", \"eval_recall\"]\n    metrics = { i + sa: [] for i in keys for sa in [\"_opos\", \"_oneg\"]}\n    for i in range(num_iterations):\n        for subtask , training , testing , validating in zip([\"opos\", \"oneg\"], [training_opos, training_oneg], [testing_opos, testing_oneg], [validating_opos, validating_oneg]):\n            trainer = Trainer(\n                model=net(2),\n                args=training_args, \n                train_dataset=training.shuffle(seed=i), \n                eval_dataset=validating, \n                compute_metrics=compute_metrics,\n                callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]\n            )\n            trainer.train()\n            res = trainer.evaluate(testing)\n            for m in keys:\n                metrics[m + \"_\" + subtask].append(res[m])\n\n    for m in metrics.keys():\n        print(\"{:18s}\\t: {:.2} ± {:.2}\".format(m, np.mean(metrics[m]), np.std(metrics[m])))\n    print(\"\\nOverall F1: {:.2}\".format(np.mean(\n        [np.mean(metrics[\"eval_f1_opos\"]), np.mean(metrics[\"eval_f1_oneg\"])]\n    )))","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:27:36.351094Z","iopub.status.idle":"2021-07-13T14:27:36.351715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\n\na = AlBERTo_Preprocessing(do_lower_case=True)\ns: str = \"#IlGOverno presenta le linee guida sulla scuola #labuonascuola - http://t.co/SYS1T9QmQN\"\nb = a.preprocess(s)\n\ntok = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\npretrained_model = AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\ntok.model_max_length = 128 #model.config.max_position_embeddings\ntokens = tok.tokenize(b)\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:27:36.352861Z","iopub.status.idle":"2021-07-13T14:27:36.353578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = \"#IlGOverno presenta le linee guida sulla scuola #labuonascuola - http://t.co/SYS1T9QmQN\"\no = pretrained_model(**tok(\"ciao \" * 1000, return_tensors=\"pt\", truncation=True))\n\no[0].shape , o[1].shape","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:27:36.355046Z","iopub.status.idle":"2021-07-13T14:27:36.355687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyNet(nn.Module):\n    \"\"\"\n        Attach a FC layer on top of the BERT head in order to produce a classification output.\n        Hyperparameters are taken from Alberto.\n\n        The pooled_output output of BERT is basically a projection of the [CLS] embeddings via another FC layer (768 -> 768 hidden units).\n        We stack another FC layer with Dropout on top of that, as reported in https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_classifier.py#L574\n    \"\"\"\n    def __init__(self, num_labels):\n        super(MyNet, self).__init__()\n\n        self.num_labels = num_labels\n        self.model = copy.deepcopy(pretrained_model)#AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(768, num_labels)\n\n        if self.num_labels >= 2:\n            self.loss_fct = nn.CrossEntropyLoss()\n        else:\n            self.loss_fct = nn.BCEWithLogitsLoss()\n\n\n    def forward(self, labels, input_ids, attention_mask, **args):\n        #For the output format -> https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForSequenceClassification.forward\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **args)\n        x = self.dropout(outputs[1])\n        logits = self.linear(x)\n\n        loss = self.loss_fct(logits, labels)\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:27:36.356897Z","iopub.status.idle":"2021-07-13T14:27:36.357493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(examples):\n    sa = tok(examples[\"text\"], padding=\"max_length\", truncation=True)\n    return sa\n\ndef separate2united_labels2(row):\n    return str(row[\"opos\"]) + str(row[\"oneg\"])\n\ndef process_dataset_task(example, subtask):\n    example[\"labels\"] = int(example[\"labels\"][0]) if subtask == \"opos\" else int(example[\"labels\"][1])\n    return example\n\n#train set\ndataset = pd.DataFrame({\"text\": train.text.apply(a.preprocess), \"idx\": train.index, \"labels\": train[[\"opos\", \"oneg\"]].apply(separate2united_labels2, axis=1)})\nX_train, X_val = train_test_split(dataset, test_size=0.2, random_state=42, stratify=dataset[\"labels\"])\n\nX_train = Dataset.from_pandas(X_train)\nX_val = Dataset.from_pandas(X_val)\n\n\n##\n# Create a Dataset for each subtask (evaluated separately by SentiPolc16).\n# To do that, simply take, respectively, the first or the second char of the label for opos and oneg.\n##\ntraining_opos = X_train\\\n                    .map(process_dataset_task, fn_kwargs={\"subtask\": \"opos\"}, batched=False)\\\n                    .map(tokenize_function, batched=True)\\\n                    .shuffle(seed=42)\\\n                    .with_format(\"torch\")\ntraining_oneg = X_train\\\n                   .map(process_dataset_task, fn_kwargs={\"subtask\": \"oneg\"}, batched=False)\\\n                   .map(tokenize_function, batched=True)\\\n                   .shuffle(seed=42)\\\n                   .with_format(\"torch\")\nvalidating_opos = X_val\\\n                    .map(process_dataset_task, fn_kwargs={\"subtask\": \"opos\"}, batched=False)\\\n                    .map(tokenize_function, batched=True)\\\n                    .with_format(\"torch\")\nvalidating_oneg = X_val\\\n                   .map(process_dataset_task, fn_kwargs={\"subtask\": \"oneg\"}, batched=False)\\\n                   .map(tokenize_function, batched=True)\\\n                   .with_format(\"torch\")\n\n\n#test set\ndataset = pd.DataFrame({\"text\": test.text.apply(a.preprocess), \"idx\": test.index, \"labels\": test[[\"opos\", \"oneg\"]].apply(separate2united_labels2, axis=1)})\ndataset = Dataset.from_pandas(dataset)\n\n##\n# Create a Dataset for each subtask (evaluated separately by SentiPolc16).\n# To do that, simply take, respectively, the first or the second char of the label for opos and oneg.\n##\ntesting_opos = dataset.map(process_dataset_task, fn_kwargs={\"subtask\": \"opos\"}, batched=False).map(tokenize_function, batched=True).shuffle().with_format(\"torch\")\ntesting_oneg = dataset\\\n                    .map(process_dataset_task, fn_kwargs={\"subtask\": \"oneg\"}, batched=False)\\\n                    .map(tokenize_function, batched=True)\\\n                    .with_format(\"torch\")","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:27:36.358816Z","iopub.status.idle":"2021-07-13T14:27:36.359411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n\nclass MemorySaverCallback(TrainerCallback):\n    \"A callback that deleted the folder in which checkpoints are saved, to save memory\"\n    def __init__(self, run_name):\n        super(MemorySaverCallback, self).__init__()\n        self.run_name = run_name\n\n    def on_train_begin(self, args, state, control, **kwargs):\n        print(\"Removing dirs...\")\n        if os.path.isdir(f'./{self.run_name}'):\n            import shutil\n            shutil.rmtree(f'./{self.run_name}')\n        else:\n            print(\"\\n\\nDirectory does not exists\")\n\nTRAIN_BATCH_SIZE = 64 \nPREDICT_BATCH_SIZE = 64\nEVAL_BATCH_SIZE = 64 \nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 3\nMAX_SEQ_LENGTH = 128\nWARMUP_PROPORTION = 0.1\nnum_train_steps = int(len(training_opos) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)+1\nNUM_WARMUP_STEPS =  int(NUM_TRAIN_EPOCHS * WARMUP_PROPORTION)\nRUN_NAME = \"test_trainer\"","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:27:36.360554Z","iopub.status.idle":"2021-07-13T14:27:36.361295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline with train-val splitting","metadata":{}},{"cell_type":"code","source":"args = TrainingArguments(\n                \"test_trainer\", \n                num_train_epochs=15,\n                per_device_train_batch_size=TRAIN_BATCH_SIZE,\n                per_device_eval_batch_size=PREDICT_BATCH_SIZE,\n                save_total_limit=2,\n                learning_rate=LEARNING_RATE,\n                warmup_steps=NUM_WARMUP_STEPS,\n                weight_decay=0.01,\n                adam_beta1=0.9,\n                adam_beta2=0.999,\n                adam_epsilon=1e-6,\n                evaluation_strategy=\"epoch\",\n                logging_strategy=\"epoch\", #before was 'step', check this\n                logging_first_step=False,\n                overwrite_output_dir=True,\n                save_strategy=\"no\",\n                report_to=\"none\",\n                load_best_model_at_end=True,  \n                metric_for_best_model=\"eval_loss\",\n            )\n\nk_steps_evidence(num_iterations=5, \n                 early_stopping_patience=3,\n                 training_args=args, \n                 net=MyNet, \n                 training_opos=training_opos, \n                 training_oneg=training_oneg, \n                 validating_opos=validating_opos,\n                 validating_oneg=validating_oneg,\n                 testing_opos=testing_opos, \n                 testing_oneg=testing_oneg)            ","metadata":{"execution":{"iopub.status.busy":"2021-07-13T14:29:17.557362Z","iopub.execute_input":"2021-07-13T14:29:17.557720Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='292' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 292/1395 05:07 < 19:30, 0.94 it/s, Epoch 3.13/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.484700</td>\n      <td>0.410385</td>\n      <td>0.821188</td>\n      <td>0.768502</td>\n      <td>0.779933</td>\n      <td>0.759670</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.347200</td>\n      <td>0.415323</td>\n      <td>0.809717</td>\n      <td>0.769663</td>\n      <td>0.762605</td>\n      <td>0.778852</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.229500</td>\n      <td>0.430693</td>\n      <td>0.827935</td>\n      <td>0.785527</td>\n      <td>0.784892</td>\n      <td>0.786174</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='465' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 465/1395 08:22 < 16:49, 0.92 it/s, Epoch 5/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.526000</td>\n      <td>0.475239</td>\n      <td>0.767881</td>\n      <td>0.748846</td>\n      <td>0.767177</td>\n      <td>0.742418</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.367900</td>\n      <td>0.451778</td>\n      <td>0.784750</td>\n      <td>0.776958</td>\n      <td>0.776209</td>\n      <td>0.777801</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.257300</td>\n      <td>0.465462</td>\n      <td>0.792173</td>\n      <td>0.783779</td>\n      <td>0.784099</td>\n      <td>0.783471</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.148900</td>\n      <td>0.631098</td>\n      <td>0.786100</td>\n      <td>0.778357</td>\n      <td>0.777603</td>\n      <td>0.779204</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.075900</td>\n      <td>0.799730</td>\n      <td>0.785425</td>\n      <td>0.778931</td>\n      <td>0.777108</td>\n      <td>0.781637</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [32/32 00:08]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='465' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 465/1395 08:20 < 16:45, 0.92 it/s, Epoch 5/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.504700</td>\n      <td>0.394078</td>\n      <td>0.813765</td>\n      <td>0.744409</td>\n      <td>0.780025</td>\n      <td>0.725921</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.360800</td>\n      <td>0.381903</td>\n      <td>0.825911</td>\n      <td>0.778438</td>\n      <td>0.784121</td>\n      <td>0.773478</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.248300</td>\n      <td>0.411073</td>\n      <td>0.823887</td>\n      <td>0.783046</td>\n      <td>0.779375</td>\n      <td>0.787141</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.155900</td>\n      <td>0.478996</td>\n      <td>0.829960</td>\n      <td>0.772665</td>\n      <td>0.798070</td>\n      <td>0.756696</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.090900</td>\n      <td>0.639659</td>\n      <td>0.809717</td>\n      <td>0.772114</td>\n      <td>0.763243</td>\n      <td>0.784877</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [32/32 00:08]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='372' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 372/1395 06:40 < 18:26, 0.92 it/s, Epoch 4/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.521800</td>\n      <td>0.449956</td>\n      <td>0.788799</td>\n      <td>0.782865</td>\n      <td>0.780771</td>\n      <td>0.786370</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.373900</td>\n      <td>0.472580</td>\n      <td>0.784076</td>\n      <td>0.780620</td>\n      <td>0.779466</td>\n      <td>0.789501</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.252900</td>\n      <td>0.537459</td>\n      <td>0.786775</td>\n      <td>0.778764</td>\n      <td>0.778332</td>\n      <td>0.779224</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.144900</td>\n      <td>0.616400</td>\n      <td>0.789474</td>\n      <td>0.786178</td>\n      <td>0.785059</td>\n      <td>0.795384</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [32/32 00:08]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='344' max='1395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 344/1395 05:56 < 18:14, 0.96 it/s, Epoch 3.69/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.489500</td>\n      <td>0.387546</td>\n      <td>0.831309</td>\n      <td>0.783222</td>\n      <td>0.792421</td>\n      <td>0.775703</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.348600</td>\n      <td>0.414078</td>\n      <td>0.818489</td>\n      <td>0.754411</td>\n      <td>0.783927</td>\n      <td>0.737470</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.236200</td>\n      <td>0.432762</td>\n      <td>0.831984</td>\n      <td>0.771817</td>\n      <td>0.805126</td>\n      <td>0.752824</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Hyper-parameter tuning","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    RUN_NAME, \n    num_train_epochs=15,\n    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size=PREDICT_BATCH_SIZE,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=1,\n    logging_first_step=False,\n    overwrite_output_dir=True,\n    save_strategy=\"no\",\n    save_total_limit=1,\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    run_name=\"alberto-repr-kaggle-opos-7\"\n)\n\nnp.random.seed(0)\ntorch.manual_seed(0)\ntrainer = Trainer(\n    model_init=partial(MyNet,2),\n    args=training_args, \n    train_dataset=training_opos, \n    eval_dataset=validating_opos,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), MemorySaverCallback(RUN_NAME)]\n)\n\n\ndef my_hp_space_optuna(trial):\n    return {\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 2e-6, 2e-4, log=True),\n        \"warmup_steps\":  trial.suggest_float(\"warmup_steps\", 0., 0.9, step=0.3),\n        \"weight_decay\":  trial.suggest_float(\"weight_decay\", 1e-6, 1e-1)\n    }\ndef my_objective(metrics):\n    return metrics[\"eval_f1\"]\n\nsa = trainer.hyperparameter_search(\n    direction=\"maximize\", \n    n_trials=10,\n    hp_space=my_hp_space_optuna, \n    compute_objective=my_objective\n)\n\n#trainer.evaluate()\n#trainer.train()\n#wandb.finish()\nsa","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments(\n                \"test_trainer\", \n                num_train_epochs=15,\n                per_device_train_batch_size=TRAIN_BATCH_SIZE,\n                per_device_eval_batch_size=PREDICT_BATCH_SIZE,\n                save_total_limit=2,\n                learning_rate=sa[\"learning_rate\"],\n                warmup_steps=sa[\"warmup_steps\"],\n                weight_decay=sa[\"weight_decay\"],\n                adam_beta1=0.9,\n                adam_beta2=0.999,\n                adam_epsilon=1e-6,    \n                valuation_strategy=\"epoch\",\n                logging_strategy=\"steps\",\n                logging_steps=1,\n                logging_first_step=False,\n                overwrite_output_dir=True,\n                save_strategy=\"no\",\n                report_to=\"none\",\n                load_best_model_at_end=True,  \n                metric_for_best_model=\"eval_loss\",\n            )\n\nk_steps_evidence(num_iterations=5, \n                 early_stopping_patience=3,\n                 training_args=args, \n                 net=MyNet, \n                 training_opos=training_opos, \n                 training_oneg=training_oneg, \n                 validating_opos=validating_opos,\n                 validating_oneg=validating_oneg,\n                 testing_opos=testing_opos, \n                 testing_oneg=testing_oneg)            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Architecture exploration","metadata":{}},{"cell_type":"markdown","source":"#### Add BatchNormalization to classification head","metadata":{}},{"cell_type":"code","source":"class BertPoolerWithBN(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.LeakyReLU()\n        self.bn = nn.LayerNorm(config.hidden_size)\n\n    def forward(self, hidden_states):\n        # We \"pool\" the model by simply taking the hidden state corresponding to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.bn(pooled_output)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n    \n    \nclass MyNetBN(nn.Module):\n    \"\"\"\n        Attach a FC layer on top of the BERT head in order to produce a classification output.\n        Hyperparameters are taken from Alberto.\n\n        The pooled_output output of BERT is basically a projection of the [CLS] embeddings via another FC layer (768 -> 768 hidden units).\n        We stack another FC layer with Dropout on top of that, as reported in https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_classifier.py#L574\n    \"\"\"\n    def __init__(self, num_labels):\n        super(MyNetBN, self).__init__()\n\n        self.num_labels = num_labels\n        self.model = copy.deepcopy(pretrained_model)#AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(768, num_labels)\n        \n        self.model.pooler = BertPoolerWithBN(self.model.config)\n\n        if self.num_labels >= 2:\n            self.loss_fct = nn.CrossEntropyLoss()\n        else:\n            self.loss_fct = nn.BCEWithLogitsLoss()\n\n\n    def forward(self, labels, input_ids, attention_mask, **args):\n        #For the output format -> https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForSequenceClassification.forward\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **args)\n        x = self.dropout(outputs[1])\n        logits = self.linear(x)\n\n        loss = self.loss_fct(logits, labels)\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{"execution":{"iopub.status.busy":"2021-07-12T08:57:59.467001Z","iopub.execute_input":"2021-07-12T08:57:59.467375Z","iopub.status.idle":"2021-07-12T08:57:59.480291Z","shell.execute_reply.started":"2021-07-12T08:57:59.467341Z","shell.execute_reply":"2021-07-12T08:57:59.477761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RUN_NAME = \"BN_NET\"\n\ntraining_args = TrainingArguments(\n    RUN_NAME, \n    num_train_epochs=15,\n    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size=PREDICT_BATCH_SIZE,\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=1,\n    logging_first_step=True,\n    overwrite_output_dir=True,\n    save_strategy=\"no\",\n    save_total_limit=1,\n    learning_rate=4.723383529363845e-06,\n    warmup_steps=0.6,\n    weight_decay=0.009760393798851559,\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    adam_epsilon=1e-6,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_f1\",\n    report_to=\"none\",\n    run_name=\"alberto-repr-kaggle-opos-9-hyper-tuning1_MyNetBN\"\n)\n\nnp.random.seed(0)\ntorch.manual_seed(0)\ntrainer = Trainer(\n    model_init=partial(MyNetBN, 2),\n    args=training_args, \n    train_dataset=training_opos, \n    eval_dataset=validating_opos,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\nprint(trainer.evaluate())\ntrainer.train()\nprint(trainer.evaluate())\nprint(trainer.predict(testing_opos))\n#wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T08:58:00.547437Z","iopub.execute_input":"2021-07-12T08:58:00.547766Z","iopub.status.idle":"2021-07-12T09:18:21.361803Z","shell.execute_reply.started":"2021-07-12T08:58:00.547733Z","shell.execute_reply":"2021-07-12T09:18:21.360872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AlBERTo baseline with val set\n'test_f1': 0.7281719646005023\n--------------------------------------\nlearning_rate=4.723383529363845e-06,\nwarmup_steps=0.6,\nweight_decay=0.009760393798851559\n1\t0.475600\t0.476718\t0.782051\t0.637935\t0.797978\t0.624925\n2\t0.443400\t0.406835\t0.823212\t0.742419\t0.816199\t0.715883\n3\t0.507200\t0.385035\t0.824561\t0.775645\t0.782895\t0.769533\n4\t0.233800\t0.380086\t0.836032\t0.781769\t0.805750\t0.766165\n5\t0.297100\t0.386492\t0.834683\t0.789434\t0.795611\t0.784060\n6\t0.172400\t0.398375\t0.834683\t0.785256\t0.798699\t0.775023\n7\t0.105000\t0.412933\t0.823212\t0.782684\t0.778505\t0.787427\n'test_f1': 0.7589981278875193\n--------------------------------------\nMyNetBN\nlearning_rate=4.723383529363845e-06,\nwarmup_steps=0.6,\nweight_decay=0.009760393798851559\n1\t0.646700\t0.594625\t0.696356\t0.673514\t0.679184\t0.722333\n2\t0.571800\t0.464716\t0.799595\t0.751313\t0.749690\t0.753028\n3\t0.598400\t0.430946\t0.824561\t0.779153\t0.781366\t0.777064\n4\t0.398500\t0.404444\t0.833333\t0.774075\t0.806625\t0.755263\n5\t0.507500\t0.423892\t0.821188\t0.769649\t0.779302\t0.761929\n6\t0.359600\t0.430221\t0.826586\t0.772422\t0.789431\t0.760389\n'test_f1': 0.685691933390074\n--------------------------------------\nMyNetBN with LayerNorm instead\nlearning_rate=4.723383529363845e-06,\nwarmup_steps=0.6,\nweight_decay=0.009760393798851559\n1\t0.531000\t0.463256\t0.778003\t0.649657\t0.757834\t0.634176\n2\t0.509800\t0.398287\t0.821188\t0.768887\t0.779716\t0.760423\n3\t0.287400\t0.384489\t0.830634\t0.765049\t0.809589\t0.742854\n4\t0.291200\t0.371507\t0.838057\t0.785421\t0.807558\t0.770577\n5\t0.317900\t0.378747\t0.834008\t0.780444\t0.801411\t0.766272\n6\t0.336200\t0.388687\t0.832659\t0.788032\t0.792307\t0.784167\n7\t0.149800\t0.390517\t0.836032\t0.789463\t0.798551\t0.781980\n8\t0.258800\t0.434438\t0.813090\t0.777745\t0.767678\t0.793234\n9\t0.254100\t0.425094\t0.833333\t0.790678\t0.792266\t0.789152\n10\t0.145100\t0.441016\t0.835358\t0.789790\t0.796821\t0.783773\n11\t0.288900\t0.450081\t0.831309\t0.788940\t0.789380\t0.788506\n12\t0.186100\t0.465647\t0.833333\t0.790357\t0.792417\t0.788399\n'test_f1': 0.7224176465802333","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### GRU over all hidden states","metadata":{}},{"cell_type":"code","source":"class BertPoolerWithBN(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.gru = nn.GRU(input_size=config.hidden_size, \n                          hidden_size=config.hidden_size, \n                          bidirectional=True, \n                          dropout=0.1,\n                          num_layers=2)\n        self.activation = nn.LeakyReLU()\n        self.bn = nn.LayerNorm(config.hidden_size)\n\n    def forward(self, hidden_states):\n        # We \"pool\" the model by simply taking the hidden state corresponding to the first token.\n        first_token_tensor = hidden_states[:, :]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.bn(pooled_output)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n    \n    \nclass MyNetGRU(nn.Module):\n    \"\"\"\n        Attach a FC layer on top of the BERT head in order to produce a classification output.\n        Hyperparameters are taken from Alberto.\n\n        The pooled_output output of BERT is basically a projection of the [CLS] embeddings via another FC layer (768 -> 768 hidden units).\n        We stack another FC layer with Dropout on top of that, as reported in https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_classifier.py#L574\n    \"\"\"\n    def __init__(self, num_labels):\n        super(MyNetBN, self).__init__()\n\n        self.num_labels = num_labels\n        self.model = copy.deepcopy(pretrained_model)#AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(768, num_labels)\n        \n        self.model.pooler = BertPoolerWithBN(self.model.config)\n\n        if self.num_labels >= 2:\n            self.loss_fct = nn.CrossEntropyLoss()\n        else:\n            self.loss_fct = nn.BCEWithLogitsLoss()\n\n\n    def forward(self, labels, input_ids, attention_mask, **args):\n        #For the output format -> https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForSequenceClassification.forward\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **args)\n        x = self.dropout(outputs[1])\n        logits = self.linear(x)\n\n        loss = self.loss_fct(logits, labels)\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","metadata":{},"execution_count":null,"outputs":[]}]}