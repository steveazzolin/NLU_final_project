{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2f63a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=nlu_sentiment_analysis\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/steve/.netrc\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=nlu_sentiment_analysis\n",
    "!wandb login 2cad8a8279143c69ce071f54bf37c1f5a5f4e5ff\n",
    "import wandb\n",
    "\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import requests, re, string, datetime, copy\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T, torch.nn.functional as F, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback, TrainerCallback\n",
    "from transformers import Trainer, get_linear_schedule_with_warmup\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "PATH = \"./data/Sentipolc16/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f12392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(PATH + \"training_set_sentipolc16.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d9bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(PATH + \"test_set_sentipolc16_gold2000.csv\", 'r') \n",
    "Lines = file1.readlines()\n",
    " \n",
    "test = []\n",
    "for line in Lines:\n",
    "  arr = line.split(\"\\\",\")\n",
    "  if len(arr) != 9:\n",
    "    arr[8] = arr[8] + arr[9]  #to account for tweets containing the delimiter charachter that would create more splits than needed\n",
    "    del arr[9:]\n",
    "  for i in range(8):\n",
    "    arr[i] = int(arr[i].strip(\"\\\"\"))\n",
    "  test.append(arr)\n",
    "\n",
    "test = pd.DataFrame(test, columns=train.columns)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54754535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tokenization classes for Italian AlBERTo models.\"\"\"\n",
    "import collections\n",
    "import os\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip(\"\\n\")\n",
    "        vocab[token] = index\n",
    "    return vocab\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'user', 'percent', 'money', 'phone', 'time', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\"},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "\n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "class AlBERTo_Preprocessing(object):\n",
    "    def __init__(self, do_lower_case=True, **kwargs):\n",
    "        self.do_lower_case = do_lower_case\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if self.do_lower_case:\n",
    "            text = text.lower()\n",
    "        text = str(\" \".join(text_processor.pre_process_doc(text)))\n",
    "        text = re.sub(r'[^a-zA-ZÀ-ú</>!?♥♡\\s\\U00010000-\\U0010ffff]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
    "        text = re.sub(r'^\\s', '', text)\n",
    "        text = re.sub(r'\\s$', '', text)\n",
    "        return text\n",
    "\n",
    "a = AlBERTo_Preprocessing(do_lower_case=True)\n",
    "s = \"#IlGOverno presenta le linee guida sulla scuola #labuonascuola - http://t.co/SYS1T9QmQN\"\n",
    "b = a.preprocess(s)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a01b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    sa = tok(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    return sa\n",
    "\n",
    "a = AlBERTo_Preprocessing(do_lower_case=True)\n",
    "s: str = \"#IlGOverno presenta le linee guida sulla scuola #labuonascuola - http://t.co/SYS1T9QmQN\"\n",
    "b = a.preprocess(s)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "pretrained_model = AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "tok.model_max_length = 128 #model.config.max_position_embeddings\n",
    "tokens = tok.tokenize(b)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetMC(nn.Module):\n",
    "    \"\"\"\n",
    "        Attach a FC layer on top of the BERT head in order to produce a classification output.\n",
    "\n",
    "        The pooled_output output of BERT is basically a projection of the [CLS] embeddings via another FC layer (768 -> 768 hidden units).\n",
    "        We stack another FC layer with Dropout on top of that, as reported in https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_classifier.py#L574\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels):\n",
    "        super(MyNetMC, self).__init__()\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.model = copy.deepcopy(pretrained_model)#AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.linear1 = nn.Linear(768, 3)\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, labels, input_ids, attention_mask, **args):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **args)\n",
    "        x = self.dropout1(outputs[1])\n",
    "        logits = self.linear1(x)\n",
    "        loss = self.loss_fct(logits, labels)\n",
    "        return logits , loss\n",
    "    \n",
    "class MyNetMCTuned(nn.Module):\n",
    "    \"\"\"\n",
    "        Attach a FC layer on top of the BERT head in order to produce a classification output.\n",
    "\n",
    "        The pooled_output output of BERT is basically a projection of the [CLS] embeddings via another FC layer (768 -> 768 hidden units).\n",
    "        We stack another FC layer without Dropout on top of that, as reported in https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/run_classifier.py#L574\n",
    "    \"\"\"\n",
    "    def __init__(self, num_labels):\n",
    "        super(MyNetMCTuned, self).__init__()\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.model = copy.deepcopy(pretrained_model)\n",
    "        self.linear1 = nn.Linear(768, 3)\n",
    "\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, labels, input_ids, attention_mask, **args):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **args)\n",
    "        logits = self.linear(outputs[1])\n",
    "        loss = self.loss_fct(logits, labels)\n",
    "        return logits , loss\n",
    "    \n",
    "class EarlyStopping():    \n",
    "    def __init__(self, min_delta = 0, patience = 0):        \n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = -np.Inf\n",
    "        self.stop_training = False\n",
    "    \n",
    "    def on_epoch_end(self, epoch, current_value):\n",
    "        if np.greater((current_value - self.min_delta), self.best):\n",
    "            self.best = current_value\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait > self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.stop_training = True\n",
    "        return self.stop_training\n",
    "\n",
    "\n",
    "TRAIN_BATCH_SIZE = 64 \n",
    "PREDICT_BATCH_SIZE = 64\n",
    "EVAL_BATCH_SIZE = 64 \n",
    "WEIGHT_DECAY = 0.01\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 128\n",
    "WARMUP_PROPORTION = 0.1\n",
    "num_train_steps = int(len(training) / TRAIN_BATCH_SIZE * NUM_EPOCHS)+1\n",
    "NUM_WARMUP_STEPS =  int(num_train_steps * WARMUP_PROPORTION)\n",
    "RUN_NAME = \"test_trainer\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28651925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, epoch, logging):\n",
    "    model.train()\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    cumulative_loss = 0.\n",
    "    for i , data in tqdm(enumerate(train_loader, 0), total=len(train_loader)):        \n",
    "        targets.extend(data[\"labels\"].numpy())\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in data.items()}\n",
    "        logits , loss = model(**batch)\n",
    "\n",
    "        cumulative_loss += loss.detach()\n",
    "        if (i+1) % 25 == 0 and logging:\n",
    "            print(f'Epoch: {epoch}, Loss:  {cumulative_loss.item()/i}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        outputs.extend(logits.argmax(-1).cpu().detach().numpy().tolist())\n",
    "    if logging: wandb.log({\"train\": {'loss': cumulative_loss.item() / len(outputs)}})\n",
    "    return outputs, targets\n",
    "    \n",
    "        \n",
    "def validation_epoch(model, epoch, val_loader, kind, logging):\n",
    "    model.eval()\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    cumulative_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(val_loader, 0):\n",
    "            batch = {k: v.to(device) for k, v in data.items()}\n",
    "            logits , loss = model(**batch)\n",
    "            cumulative_loss += loss.detach()\n",
    "            if (i+1) % 25 == 0 and logging:\n",
    "                print(f'Epoch: {epoch}, Loss:  {cumulative_loss.item()/i}')\n",
    "            \n",
    "            targets.extend(batch[\"labels\"].cpu().detach().numpy())\n",
    "            outputs.extend(logits.argmax(-1).cpu().detach().numpy().tolist())\n",
    "    if logging: wandb.log({kind: {'loss': cumulative_loss.item() / len(outputs)}})\n",
    "    return outputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04de4d3",
   "metadata": {},
   "source": [
    "# SENTIPOLC16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955f92ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate2united_labels(row):\n",
    "    \"\"\"\n",
    "        Return a single scalar integer label associated to the polarity of the tweet.\n",
    "\n",
    "        Negative -> 0\n",
    "        Neutral  -> 1\n",
    "        Positive -> 2\n",
    "        Mixed    -> 3\n",
    "    \"\"\"\n",
    "    if row[\"opos\"] == 0 and row[\"oneg\"] == 0:\n",
    "        return 1\n",
    "    elif row[\"oneg\"] == 0 and row[\"opos\"] == 1:\n",
    "        return 2\n",
    "    elif row[\"oneg\"] == 1 and row[\"opos\"] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "#train set\n",
    "dataset = pd.DataFrame({\"text\": train.text.apply(a.preprocess), \"idx\": train.index, \"labels\": train[[\"opos\", \"oneg\"]].apply(separate2united_labels, axis=1)})\n",
    "X_train, X_val = train_test_split(dataset, test_size=0.2, random_state=42, stratify=dataset[\"labels\"])\n",
    "\n",
    "X_train = Dataset.from_pandas(X_train)\n",
    "X_val = Dataset.from_pandas(X_val)\n",
    "\n",
    "training = X_train\\\n",
    "                    .map(tokenize_function, batched=True)\\\n",
    "                    .filter(lambda example: example['labels'] != 3)\\\n",
    "                    .shuffle(seed=42)\\\n",
    "                    .with_format(\"torch\")\n",
    "validating = X_val\\\n",
    "                    .map(tokenize_function, batched=True)\\\n",
    "                    .filter(lambda example: example['labels'] != 3)\\\n",
    "                    .with_format(\"torch\")\n",
    "\n",
    "\n",
    "#test set\n",
    "dataset = pd.DataFrame({\"text\": test.text.apply(a.preprocess), \"idx\": test.index, \"labels\": test[[\"opos\", \"oneg\"]].apply(separate2united_labels, axis=1)})\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "testing = dataset\\\n",
    "                    .map(tokenize_function, batched=True)\\\n",
    "                    .filter(lambda example: example['labels'] != 3)\\\n",
    "                    .with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6061b17d",
   "metadata": {},
   "source": [
    "### AlBERTo pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d15e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(testing.with_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\"]), batch_size=64)\n",
    "preds , trues = validation_epoch(model, None, test_loader, \"test\", logging=False)\n",
    "\n",
    "dataset_test = pd.DataFrame({\"text\": test.text.apply(a.preprocess), \"idx\": test.index, \"labels\": test[[\"opos\", \"oneg\"]].apply(separate2united_labels, axis=1)})\n",
    "dataset_test = dataset_test[dataset_test.labels != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cdf09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best model's params\n",
    "model = MyNetMCTuned(3).to(device)\n",
    "model.load_state_dict(torch.load(\"data/models/alberto_multiclass.pt\"))\n",
    "\n",
    "preds , trues = validation_epoch(model, None, test_loader, \"test\", logging=False)\n",
    "\n",
    "for n , (i , row) in enumerate(dataset_test.iterrows()):\n",
    "    if preds[n] != trues[n]:\n",
    "        print(f\"{row['text']} ------ true={row['labels']} pred={preds[n]}  \\n\")\n",
    "        assert row['labels'] == trues[n]\n",
    "        \n",
    "print(classification_report(trues, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "cm = confusion_matrix(trues, preds, normalize=True)\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"negative\", \"neutral\", \"positive\"]).plot(values_format=\"d\")\n",
    "plt.title(\"Sentipolc16 - AlBERToMC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb519f7",
   "metadata": {},
   "source": [
    "### AlBERTo fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db4c73",
   "metadata": {},
   "source": [
    "# FEEL-IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e1b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8df350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
